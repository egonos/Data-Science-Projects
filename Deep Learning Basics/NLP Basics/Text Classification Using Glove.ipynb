{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8DLjLQmK4B3I"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "0ec8822f99384882917a8ba601ed11d6",
            "5168856d0cd8409e99645e46f5a253f0",
            "34a1456a4edd4787ab718f41ca380d40",
            "27117719e93d436bac0d8e1f55e59e69",
            "fdd23537eb1041f38f9abbc9274ec7d6",
            "c056bc9a930b49a092a57cc7570d0dc1",
            "d0e7ca5b414b410f9bfed5c531504eca",
            "97739ac3c20c4e3fbf46ca8e5ba5a7cf",
            "e71ee393290b403d87414bf9c0bc5525",
            "dcf62433e2894472b99cf2f3a21efb3d",
            "f809af5f6b2d416aaa3000bc13584368",
            "e6c96959d334468ab9a39620f8bee888",
            "5bf57cacf8b44532b81959a4bb5f3630",
            "dd1aaccb18c74eb6a04d0c62e3aa4b7e",
            "93579b95b5f24253a63021e53ffbc2a5",
            "0424e791ac9d4329885d0316f9bc6b00",
            "6c534ff8c71946b8b5a73d13175eb643",
            "d7f7397b6bba4b4dbe6626895d7da0bc",
            "71e4dca79660492695fcf5a896624f9a",
            "29a8732e8dc14c3a832da9898530a6f5",
            "e8e104af9e5043a084943f7fef377de2",
            "6a38c78c52654bd5b93c10850130a83b",
            "f48bcbf50c9440afbea3ab919e86df61",
            "6d5fa2ab2f40445ca1b1b83a0de7a2fe",
            "449ecb53c6d3452ba739e66061d597e5",
            "7c335548d7914881979e32db8231c97d",
            "d4fa85b27c2e49c0bb01c942e6546614",
            "5fbe250ea04c4296bf4ccbd4427d2daf",
            "4aefa220bede41f6a30e7242ce251586",
            "99b372d6c6fe43caae2cfb2e8c6a14ec",
            "e231858abc6844a1b67f91cded599d79",
            "d68253f892db47f3887957bbbe08716b",
            "d2ff58c4ae3e4b059497a70c89f9fcbf",
            "426f9379b9064ef2aee1f482995794d2",
            "2f2a77c2a1db4e87986a14e7ef96003a",
            "688cec9bc06e482fa16d71605cdbb98f",
            "97c6f15fe29b4820a1cae8cf8e38ef06",
            "9a8aea82af1e4fbaad171b0fbb6f52c9",
            "74cb99315b4f4938bb62122b3ab485ec",
            "2c346368528045b39cc934eca27e4ac2",
            "c202f42a946146ca809145e15a8f3245",
            "211642b6158c4999abe7dbede7aa6a54",
            "ef06deaadb5942a895058bff56762022",
            "637303bb7148472198acad3ed41b8fa1",
            "ea1bdd8205704526b1ed116e825acf19",
            "aa5c3b0ea9024d1a859e9f0a11cfefd5",
            "7f88a80a6d3440b9be2f253be709e5d7",
            "1c21a692c7d34062a3ba8c8aafd6d3c8",
            "8148e0eb52cf43eeb6a70f3f3979e3da",
            "caf252e6bf304114a2780393bd393096",
            "20daa3e52e82458faf49305cdb97ea89",
            "08375e4dc9b940edaac6316730fb2d9f",
            "9690062394f24b5783ea011217b6e11a",
            "69b76304059f4524be50877a671740c2",
            "eba2c90c8bdf48ccbf8760a7a0a68da9",
            "30315664562746d6811ccd8569b4fab5",
            "45a52afbde0b44b59979d619b75922e9",
            "39e0afef63934f48b1fcaf7baf6da2fb",
            "8447e0dd4f2c4f639c5f4f01ea0e2be0",
            "399ba445500c45378decc9e02a259a87",
            "72d3a5a1e1c6404cbeff7dc4f4e18a04",
            "339832d71d574facae4edcc56d9acef2",
            "3df4bd4ec4d24cb0bc99966393cc733e",
            "241c6a9b3a4d48fd931fd23458427dee",
            "ca7daba47e7a4861880c52bd34066272",
            "d4d5700e665f4b3aa18497250fabdbd0",
            "6fb1c85a218b4c5193ef015df9510322",
            "d9a89713a98642d2bdf3f3a319d18fd3",
            "5e88c41a861b4970ad0c9e7621c2761a",
            "e820b98823414a93bfd2dc73414c82ef",
            "232a724965fa4da785d0ca3b180a4bb5",
            "762bd4e71eb145ca8903d69c9cf4c6e0",
            "d26c70bb0e49438a9eca48206088dbce",
            "988cfc7eb2924e9eaca9018891efde7f",
            "60c15782562d4c3ea2524847a91d6fe8",
            "4b4ce719399244a4afc98f1f719a8d13",
            "65660dcc6c1d44ae848456ad212bb0af",
            "cd4fce53796e4dae96afd882ba6bd392",
            "cc7a9103697e44e2b15549a352633643",
            "b08163bc5b634aa88a0cd24ed884e071",
            "44b3f8396a2c48b9989c438ce6e2744e",
            "5e98451337a34a04b7f86c20c1151ccb",
            "b5030130c3264007bf7d77fabb842b91",
            "19121bb72cab487399b2b5bd5c073e3e",
            "b24ea9f36692415c8562181038b1b3f4",
            "f9954adc75a748ceb46491e1653c909c",
            "f570707460d64ec1987016e39f91f90a",
            "3680b3e2043e4233a2b9651bad0f8af3",
            "15fcd61b81b540018a1c6ed1d73f0e92",
            "f660778223904490b33db604b82ae22b",
            "818d50bdfa2d4fa1b2a3852373408001",
            "d79717c04cf54fb181b8395a3882654d",
            "bdc53f5d83fb4043a758f36502554012",
            "8234f9881fb749dd91a84c32f140fed9",
            "0cac7d619ab849e38b39a223f6c82a58",
            "d85687433e1447c2b13373a2cf3e63ca",
            "d5d1fa62ef3d40d980a4697a6bcb5fb7",
            "45aeb42196bb4c41aa199d588009b0c1",
            "9c93815f350642ed84527ac30c7d2d90"
          ]
        },
        "id": "u9lRFmqs4ay8",
        "outputId": "384a7fe1-f9f4-4213-97c2-13488a4ba8b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ec8822f99384882917a8ba601ed11d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6c96959d334468ab9a39620f8bee888",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f48bcbf50c9440afbea3ab919e86df61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "426f9379b9064ef2aee1f482995794d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea1bdd8205704526b1ed116e825acf19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteQMTOWO/imdb_reviews-train.tfrecord\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30315664562746d6811ccd8569b4fab5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fb1c85a218b4c5193ef015df9510322",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteQMTOWO/imdb_reviews-test.tfrecord*\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd4fce53796e4dae96afd882ba6bd392",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15fcd61b81b540018a1c6ed1d73f0e92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteQMTOWO/imdb_reviews-unsupervised.t\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "(train_dataset,test_dataset),ds_info = tfds.load('imdb_reviews',\n",
        "                                                 with_info = True,\n",
        "                                         split = ['train','test'],\n",
        "                                                  as_supervised = True,\n",
        "                                         shuffle_files = True,\n",
        "                                         batch_size = 128\n",
        "                                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP4ZnqY_5c_v",
        "outputId": "938ff76c-6b80-4231-c305-b320d24193e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-06 03:47:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-10-06 03:47:10--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-10-06 03:47:11--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: \u2018glove.6B.zip\u2019\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  3.66MB/s    in 4m 18s  \n",
            "\n",
            "2023-10-06 03:51:30 (3.18 MB/s) - \u2018glove.6B.zip\u2019 saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8JAw6UG_xRl"
      },
      "source": [
        "## Create Glove Embedding Matrix\n",
        "\n",
        "Got help from [here.](https://github.com/TheKidPadra/DeepLearning.AI-TensorFlow_Developer-specialization/blob/main/3.%20Natural%20Language%20Processing%20in%20TensorFlow/C3W3_Assignment.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fKNR4UDz9vHM"
      },
      "outputs": [],
      "source": [
        "#read the glove embeding file\n",
        "GLOVE_FILE = './glove.6B.100d.txt'\n",
        "\n",
        "#initialize a dictionary\n",
        "GLOVE_EMBEDDINGS = {}\n",
        "\n",
        "#open the glove file and process it.\n",
        "with open(GLOVE_FILE) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        GLOVE_EMBEDDINGS[word] = coefs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zarddptACDtz",
        "outputId": "0e6fc7be-58b4-47c6-8014-5512a97a48e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "725"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_lengths = []\n",
        "reviews = train_dataset.map(lambda x,y: x)\n",
        "\n",
        "for a in reviews:\n",
        "  text_lengths.append(len(a.numpy()[0].split()))\n",
        "\n",
        "OUTPUT_SEQUENCE_LENGTH = int(np.percentile(text_lengths,95))\n",
        "OUTPUT_SEQUENCE_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1QZxW29_mOm",
        "outputId": "15372f29-0c17-4498-b29f-4c6add1e4f1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'and', 'a']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#initialize a text vectorization layer to get the words\n",
        "vectorizer = layers.TextVectorization(max_tokens = 10000,\n",
        "                                      output_sequence_length = OUTPUT_SEQUENCE_LENGTH)\n",
        "\n",
        "#adapt the vectorizer\n",
        "vectorizer.adapt(train_dataset.map(lambda x,y: x))\n",
        "\n",
        "words = vectorizer.get_vocabulary()\n",
        "\n",
        "#control the process\n",
        "words[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VpmPcSJyByDL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def text_vectorization(dataset,vectorizer):\n",
        "  \"\"\"\n",
        "  Uses TextVectorization Layer outside of the model.\n",
        "  The purpose is to make the NN model suitable to .h5 format.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset: Tensorflow dataset containing data and labels.\n",
        "  vectorizer: A TextVectorization layer adapted to train_dataset\n",
        "  \"\"\"\n",
        "  #conduct all the process in CPU to CPU-GPU conflict\n",
        "  with tf.device(\"/cpu:0\"):\n",
        "    outputs = []\n",
        "    label_list = []\n",
        "\n",
        "    #tokenize and pad the data and store it\n",
        "    for x, y in dataset:\n",
        "        output = vectorizer(x)\n",
        "        outputs.append(output)\n",
        "        label_list.append(y)\n",
        "\n",
        "    #concatenate the labels and data\n",
        "    X_vectorized = tf.concat(outputs, axis=0)\n",
        "    Y_labels = tf.concat(label_list, axis=0)\n",
        "\n",
        "    vectorized = tf.data.Dataset.from_tensor_slices((X_vectorized, Y_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "    return vectorized\n",
        "\n",
        "def define_callbacks(model):\n",
        "  es = tf.keras.callbacks.EarlyStopping(patience = 5,verbose = 1, restore_best_weights = True)\n",
        "  mc = tf.keras.callbacks.ModelCheckpoint(filepath = f\"./ModelCheckpoints/{model.name}.ckpt\",\n",
        "                                         save_best_only = True,\n",
        "                                         save_weights_only = True)\n",
        "  tb = tf.keras.callbacks.TensorBoard(log_dir = f\"./TensorboardLogs/{model.name}\")\n",
        "\n",
        "  return es,mc,tb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KrBkKBZJDI0E"
      },
      "outputs": [],
      "source": [
        "train_dataset_vectorized1 = text_vectorization(train_dataset,vectorizer)\n",
        "test_dataset_vectorized1 = text_vectorization(test_dataset,vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaCraGzxDSh_",
        "outputId": "361340eb-4d39-4d24-97de-b084f5ae31c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXdQXAvIAugf"
      },
      "outputs": [],
      "source": [
        "#create a model with glove embeddings\n",
        "MAX_TOKENS = 10000\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape = (598,))\n",
        "x = layers.Embedding(input_dim = MAX_TOKENS,\n",
        "                     output_dim = EMBEDDING_DIM,\n",
        "                     embeddings_initializer=tf.keras.initializers.Constant(GLOVE_EMBEDDINGS),\n",
        "                     trainable = False)(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(1,activation = 'sigmoid')\n",
        "\n",
        "model = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "#compile the model\n",
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "#define callbacks\n",
        "es,mc,tb = define_callbacks(model)\n",
        "\n",
        "#fit the model\n",
        "history = model.fit(train_dataset_vectorized1,\n",
        "                    validation_data = test_dataset_vectorized1,\n",
        "                    epochs = 100,\n",
        "                    callbacks = [es,mc])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}