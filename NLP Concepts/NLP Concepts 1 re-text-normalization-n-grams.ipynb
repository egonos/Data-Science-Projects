{"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Welcome to the first installment of my NLP series. My main objective for creating this series is to introduce the main tasks of NLP to the reader in an understandable fashion. I will follow the order of [Speech and Language Proceesing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdfstart) by Jurafsky.  I hope the reader of this series find it useful.\n\n#1. Regex\n\nRegular expressions (or regex, for short) is a fundamental skill that every NLP practitioner has to have. We use them to search  the patterns through texts and make modifications based on our intentions. These intentions often involve text preprocessing and normalization.","metadata":{"id":"E6W6h3gcfQkm"}},{"cell_type":"code","source":"import re\nimport nltk\nimport textblob\nimport spacy\n\n\nnltk.download('wordnet')\nnltk.download('punkt')","metadata":{"id":"fmLPS33Of5KS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"af6810e8-7dc1-4429-94b2-d23050046e68"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package wordnet to /root/nltk_data...\n\n[nltk_data]   Package wordnet is already up-to-date!\n\n[nltk_data] Downloading package punkt to /root/nltk_data...\n\n[nltk_data]   Package punkt is already up-to-date!\n"},{"output_type":"execute_result","execution_count":1,"data":{"text/plain":["True"]},"metadata":{}}]},{"cell_type":"markdown","source":"Here is our first search. I'll slowly increase the complexity, so please be patient if you get bored in the first examples. The first sentence of Harry Potter:","metadata":{"id":"nxkhqZCSf-3B"}},{"cell_type":"code","source":"sentence = \"Mr and Mrs Dursley, of number four, Privet Drive, were proud to say \\\nthat they were perfectly normal, thank you very much. They \\\nwere the last people you’d expect to be involved in anything \\\nstrange or mysterious, because they just didn’t hold with such \\\nnonsense.\"","metadata":{"id":"3MPYo71sf74w"},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"re.search(\"Dursley\",sentence)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIchBb14glnp","outputId":"2d1f0768-f7bf-4602-bede-639934ad7029"},"execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":["<re.Match object; span=(11, 18), match='Dursley'>"]},"metadata":{}}]},{"cell_type":"markdown","source":"The output tells us that the word we're looing for is starts at the 11. position and ends at 18. position. Now let's say I want to find Mr and Mrs at the same time. Since we are searching muliple words, we will use `.finditer()`:","metadata":{"id":"2odVQBxag66s"}},{"cell_type":"code","source":"for word in re.finditer(\"Mr.\",sentence):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9_EbvpkgowQ","outputId":"e2d00013-e59a-4414-e3cd-f1519bd1d4e1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(0, 3), match='Mr '>\n\n<re.Match object; span=(7, 10), match='Mrs'>\n"}]},{"cell_type":"markdown","source":"`.` means find *any* character after Mr if there is. If you have noticed it have also captured the whitesapace after Mr. We can correct this:","metadata":{"id":"SuCjEQ99jbdy"}},{"cell_type":"code","source":"for word in re.finditer(\"Mr\\w{0,1}\",sentence):\n  print(word)","metadata":{"id":"D8KUQTcRiJJQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"90732d41-9e8a-4144-82cc-94b0e630b36e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(0, 2), match='Mr'>\n\n<re.Match object; span=(7, 10), match='Mrs'>\n"}]},{"cell_type":"markdown","source":"Perfect. Now the `\\w` matches any alphanumeric characters and {0,1} indicates that only match 0 or 1 occurence. Now imagine that there are multiple Mr and Mrs but I only want to capture if the Mr is in the beginning of the sentence:","metadata":{"id":"eRYduDkPj2cx"}},{"cell_type":"code","source":"trial = \"Mr Dursley Mrs Dursley, Mr Ariely.\"\nfor word in re.finditer(\"^Mr\",trial):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvXZZ33TlkwZ","outputId":"e4126a87-3391-44e8-80d9-92934bd8716b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(0, 2), match='Mr'>\n"}]},{"cell_type":"markdown","source":"or in the end of the sentence:","metadata":{"id":"gjEaO-9qma0_"}},{"cell_type":"code","source":"trial = \"Mr Dursley Mrs Dursley, Mr\"\nfor word in re.finditer(\"Mr$\",trial):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZobzggaQmKyY","outputId":"02af699b-1181-43db-f067-42c197dfbf8a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(24, 26), match='Mr'>\n"}]},{"cell_type":"markdown","source":"Maybe I want to make the search case insensitively:","metadata":{"id":"2yJKvZWim4Aj"}},{"cell_type":"code","source":"trial = \"Mr Dursley Mrs Dursley, Mr Ariely and cat sound mr.\" # Don't think about the meanings for now I've just made them up.\nfor word in re.finditer(\"[Mm]r\",trial):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8XfxT7Qmm7f","outputId":"1e096bb9-f818-4e5b-bd7f-77e34f03823c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(0, 2), match='Mr'>\n\n<re.Match object; span=(11, 13), match='Mr'>\n\n<re.Match object; span=(24, 26), match='Mr'>\n\n<re.Match object; span=(48, 50), match='mr'>\n"}]},{"cell_type":"markdown","source":"Now let's proceed with more complex patterns. Let's say I want to find all the percentages within the text.","metadata":{"id":"9w5U6oLVnpFV"}},{"cell_type":"code","source":"trial = \"20% 100% 1000% ema%\" # Don't think about the meanings for now I've just made them up.\nfor word in re.finditer(\"\\w{2,3}%\",trial):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWo_8967nEJv","outputId":"727db6c8-f6e8-4be7-94c9-b155c8694a22"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(0, 3), match='20%'>\n\n<re.Match object; span=(4, 8), match='100%'>\n\n<re.Match object; span=(10, 14), match='000%'>\n\n<re.Match object; span=(15, 19), match='ema%'>\n"}]},{"cell_type":"markdown","source":"But I don't want to catch ema%. Moreover 000 should bw 1000. So I use \\d or [0-9] (They are the same thing).","metadata":{"id":"H57-G8MhokVR"}},{"cell_type":"code","source":"trial = \"20% 100% 1000% ema%\" # Don't think about the meanings for now I've just made them up.\nfor word in re.finditer(\"\\d{2,3}%\",trial):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ytin3aCgoXec","outputId":"e1bdd745-5be2-4d6c-c904-251195e7d29f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(0, 3), match='20%'>\n\n<re.Match object; span=(4, 8), match='100%'>\n\n<re.Match object; span=(10, 14), match='000%'>\n"}]},{"cell_type":"markdown","source":"Now we solved the first part of the problem but still, we have 000 instead of 1000 in the output. So if we don't know how many digits before the percentage sign then we may fail to catch the all patterns correctly. For this situations we use `+` (one or more occurences).","metadata":{"id":"MWZk4DQAo1EY"}},{"cell_type":"code","source":"trial = \"20% 100% 1000% ema%\" # Don't think about the meanings for now I've just made them up.\nfor word in re.finditer(\"\\d+%\",trial):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QE6f_ihBozHO","outputId":"53ddb5c4-45a6-44a8-9039-49ac860bbabd"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(0, 3), match='20%'>\n\n<re.Match object; span=(4, 8), match='100%'>\n\n<re.Match object; span=(9, 14), match='1000%'>\n"}]},{"cell_type":"markdown","source":"I can do the same thing with a different pattern:","metadata":{"id":"Nm-PoVJDsrLr"}},{"cell_type":"code","source":"trial = \"20% 100% 1000% ema%\" # Don't think about the meanings for now I've just made them up.\nfor word in re.finditer(\"\\w*%\\s\",trial):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdTcvsLbpG1u","outputId":"070f763d-3e47-40be-98ae-cea1f71aa73e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(0, 4), match='20% '>\n\n<re.Match object; span=(4, 9), match='100% '>\n\n<re.Match object; span=(9, 15), match='1000% '>\n"}]},{"cell_type":"markdown","source":" \\s stands for white spaces and * stands for 0 or more occurances. Don't forget to eliminate wihtespaces afterwards.","metadata":{"id":"b5zLvTbytbbU"}},{"cell_type":"code","source":"trial = \"20% 100% 1000% ema%\" # Don't think about the meanings for now I've just made them up.\nfor word in re.finditer(\"\\w*%\\s\",trial):\n  print(word.group()[:-1])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w0Z-fDz7o8hz","outputId":"a1ec4706-da82-436c-e0af-bbce7cab4b79"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"20%\n\n100%\n\n1000%\n"}]},{"cell_type":"markdown","source":"We just saw that the ^ sign is used for indicating find the pattern if the line *starts with* thatt pattern. Alternatively, we can use ^ in square brackets ([^]) to indicate *not*. Here is an example:","metadata":{"id":"Q7YykiPgqDox"}},{"cell_type":"code","source":"trial = \" Mars, mars\" # Don't think about the meanings for now I've just made them up.\nfor word in re.finditer(\"[^A-Z]\\s\\w+\",trial):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xwTv3B9ss1-I","outputId":"66f27b49-826e-4f59-dac0-e08984eddc58"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(5, 11), match=', mars'>\n"}]},{"cell_type":"markdown","source":"Let's decode one by one.\n* [^A-Z] -> Not starts with capital letters\n* \\s -> whitespaces\n* \\w+ -> one or more occurances of alphanumeric characters\n\nSince *Mars* is violating the first rule it is not captured by regex.\n\nOne last thing that I want to show is `|` (or). Here is the most basic application:","metadata":{"id":"B08BI78WvOXk"}},{"cell_type":"code","source":"trial = \"mars,martini\" # Don't think about the meanings for now I've just made them up.\nfor word in re.finditer(\"martini|mars\",trial):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2rFsuxBDrbli","outputId":"b42bfaff-8cab-4628-a45d-9792891c34f0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(0, 4), match='mars'>\n\n<re.Match object; span=(5, 12), match='martini'>\n"}]},{"cell_type":"markdown","source":"We can combine `|` with paranthesis to define a pattern rather than a word:","metadata":{"id":"49WKIkerrkq-"}},{"cell_type":"code","source":"trial = \"mars,martini\" # Don't think about the meanings for now I've just made them up.\nfor word in re.finditer(\"(mar)(tini|s)\",trial):\n  print(word)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LCDlg5k5uSpg","outputId":"f125df3a-438e-4cca-e49d-782781a9d1c8"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":"<re.Match object; span=(0, 4), match='mars'>\n\n<re.Match object; span=(5, 12), match='martini'>\n"}]},{"cell_type":"markdown","source":"Congratulations! You've just completed the first part. In my experience, regex is easily forgotten if you don't use it regularly. For example, whenever I need to use regex, first I visit this [website](https://www.w3schools.com/python/python_regex.asp) to refresh my memory about the meanings of the symbols, and then I code my patterns.\"","metadata":{"id":"11VXIo0dr0dI"}},{"cell_type":"markdown","source":"#2. Text Normalization\n\nLanguage is a complex structure. Therefore, often we need to preprocess it before giving a text directly to the model. Here are the main preprocessing steps:\n\n\n## 1.1. Case folding\n Often refers to the lowering the text. By doing that we can overcome the probelms about case sensitivity.\n\n","metadata":{"id":"RtFU7COdyLHM"}},{"cell_type":"code","source":"sentence = \"Mr and Mrs Dursley, of number four, Privet Drive, were proud to say \\\nthat they were perfectly normal, thank you very much. They \\\nwere the last people you’d expect to be involved in anything \\\nstrange or mysterious, because they just didn’t hold with such \\\nnonsense.\"\nprint(sentence.lower())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"av7r9xMTzPHi","outputId":"9d7a7465-2fcf-4e37-d11b-08c12e9544ea"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":"mr and mrs dursley, of number four, privet drive, were proud to say that they were perfectly normal, thank you very much. they were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\n"}]},{"cell_type":"markdown","source":"##1.2. Tokenization\n\nTokenization is the term used for splitting the words. We can do it manually or by using a library. What I recommend is using a library to do this because it is often done more professionally. For illustration purposes, let's do a couple of tokenization that are not perfect:","metadata":{"id":"e6JyJ24EzWCH"}},{"cell_type":"code","source":"sentence = \"Mr and Mrs Dursley, of number four, Privet Drive, were proud to say \\\nthat they were perfectly normal, thank you very much. They \\\nwere the last people you’d expect to be involved in anything \\\nstrange or mysterious, because they just didn’t hold with such \\\nnonsense.\"\n\nprint(sentence.split())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvMejVuE2XRO","outputId":"ae1dbed6-ddc9-4d99-e6cd-7eaa86f81878"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":"['Mr', 'and', 'Mrs', 'Dursley,', 'of', 'number', 'four,', 'Privet', 'Drive,', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal,', 'thank', 'you', 'very', 'much.', 'They', 'were', 'the', 'last', 'people', 'you’d', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious,', 'because', 'they', 'just', 'didn’t', 'hold', 'with', 'such', 'nonsense.']\n"}]},{"cell_type":"markdown","source":"If you noticed the punctuations were stuck to the words. So if I use `split()` in particular, I would remove the punctuations first or split them from the words. Here is a way to exclude the punctuations using re:","metadata":{"id":"i7lobckuuBkE"}},{"cell_type":"code","source":"print(re.findall(\"\\w+\",sentence))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdKGawEO2pRq","outputId":"2678c328-86a4-4a71-9beb-0d8877dd0298"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"['Mr', 'and', 'Mrs', 'Dursley', 'of', 'number', 'four', 'Privet', 'Drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much', 'They', 'were', 'the', 'last', 'people', 'you', 'd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didn', 't', 'hold', 'with', 'such', 'nonsense']\n"}]},{"cell_type":"markdown","source":"Since punctuations are not alphanumeric, they were excluded.","metadata":{"id":"fqhl6Z3zur7g"}},{"cell_type":"code","source":"import string\n\nchar_list = []\nfor char in sentence:\n  if char in string.punctuation:\n    char_list.append(' ')\n\n  char_list.append(char)\n\npunctuation_spaced = ''.join(char_list)\nprint(punctuation_spaced)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-2NS4efNwQly","outputId":"26bc1582-0747-4242-e319-4b017740954b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":"Mr and Mrs Dursley , of number four , Privet Drive , were proud to say that they were perfectly normal , thank you very much . They were the last people you’d expect to be involved in anything strange or mysterious , because they just didn’t hold with such nonsense .\n"}]},{"cell_type":"markdown","source":"Now I can tokenize:","metadata":{"id":"HESlH73Ows9J"}},{"cell_type":"code","source":"print(punctuation_spaced.split())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7MFiRW8wv0o","outputId":"d5bcb333-25e5-4e1e-9509-109fc338daa4"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"['Mr', 'and', 'Mrs', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'you’d', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', ',', 'because', 'they', 'just', 'didn’t', 'hold', 'with', 'such', 'nonsense', '.']\n"}]},{"cell_type":"markdown","source":"Now let's proceed to some more common ways to tokenize a text. We have several modules for this task.","metadata":{"id":"f9W1hUN3w10k"}},{"cell_type":"code","source":"#use nltk tokenizer\nprint(nltk.word_tokenize(sentence))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjG0p5_U3N5f","outputId":"a95ec880-5176-41fa-bc17-69dfc178a1a3"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"['Mr', 'and', 'Mrs', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'you', '’', 'd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', ',', 'because', 'they', 'just', 'didn', '’', 't', 'hold', 'with', 'such', 'nonsense', '.']\n"}]},{"cell_type":"code","source":"#use spacy tokenizer\nnlp = spacy.load(\"en_core_web_sm\")\n\ntokenized_text = nlp(sentence)","metadata":{"id":"cGDucvbLxErj"},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"tokens = []\nfor token in tokenized_text:\n  tokens.append(token)\n\nprint(tokens)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Pt_UHXxzOJ4","outputId":"ebc07562-3da1-4fd1-a9c0-f76d62ad49e8"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":"[Mr, and, Mrs, Dursley, ,, of, number, four, ,, Privet, Drive, ,, were, proud, to, say, that, they, were, perfectly, normal, ,, thank, you, very, much, ., They, were, the, last, people, you, ’d, expect, to, be, involved, in, anything, strange, or, mysterious, ,, because, they, just, did, n’t, hold, with, such, nonsense, .]\n"}]},{"cell_type":"markdown","source":"##1.3. Lemmatization\n\nLemmatization is the process in which the word is reduced to its root or more formally,\n\n*to reduce the different forms of a word to one single form, for example, reducing \"builds\", \"building\",or \"built\" to the lemma \"build\"* [1](https://dictionary.cambridge.org/dictionary/english/lemmatize)","metadata":{"id":"zomre8vWNGqy"}},{"cell_type":"code","source":"texts = [\"builds\", \"building\", \"built\"]\n[textblob.Word(text).lemmatize() for text in texts]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VI3Bv9YNSoF","outputId":"51f30036-ee87-4484-fbda-f347a9dd0254"},"execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":["['build', 'building', 'built']"]},"metadata":{}}]},{"cell_type":"markdown","source":"Now if you have noticed, the results are not the same as the dictionary output. This is just because we didn't specify the type of the word. Is it noun, verb, or adjective? Since we did not include our preferences, the algorithm ran in default mode which is noun. We know that this should be a verb. So let's change accordingly and get the results.","metadata":{"id":"HgHIdDUp0PCD"}},{"cell_type":"code","source":"texts = [\"builds\", \"building\", \"built\"]\n[textblob.Word(text).lemmatize('v') for text in texts]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x6DKXlyZ1JZj","outputId":"2ac6fec4-eb55-4bb1-cb1c-3f4a9c47cac2"},"execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":["['build', 'build', 'build']"]},"metadata":{}}]},{"cell_type":"markdown","source":"##1.4. Stemming\n\nStemming is similar to the lemmatization but instead of using a dictionary, it uses heuristics to chop the words. This may result in words having no actual meaning. To compare these two, I picked 10 random *nouns* from a [random noun generator](https://randomwordgenerator.com/noun.php) and processed the words in both ways:","metadata":{"id":"Vt2xyuWQOcTa"}},{"cell_type":"code","source":"nouns = [\"industry\", \"agency\",\"hearing\",\"promotion\",\"opportunity\",\"mom\",\"manufacturer\",\"database\",\"skill\",\"hotel\"]\nprint(\"LEMMA ----- STEM\\n\")\nfor noun in nouns:\n  print(textblob.Word(noun).lemmatize(),\"-----\",textblob.Word(noun).stem())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NlwzWp22PG6","outputId":"17ec7cc2-9d0b-4329-eca1-a61e607c2f8b"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":"LEMMA ----- STEM\n\n\n\nindustry ----- industri\n\nagency ----- agenc\n\nhearing ----- hear\n\npromotion ----- promot\n\nopportunity ----- opportun\n\nmom ----- mom\n\nmanufacturer ----- manufactur\n\ndatabase ----- databas\n\nskill ----- skill\n\nhotel ----- hotel\n"}]},{"cell_type":"markdown","source":"##1.5. Sentence Segmentation\n\nSeence segmentation is nothing but splitting the text into sentences. Again several modules supoort this. Here is how you can do it on textblob:","metadata":{"id":"f_12J3zn3jN6"}},{"cell_type":"code","source":"text = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say \\\nthat they were perfectly normal, thank you very much. They were the last \\\npeople you'd expect to be involved in anything strange or mysterious, \\\nbecause they just didn't hold with such nonsense. \\\nMr. Dursley was the director of a firm called Grunnings, which made \\\ndrills. He was a big, beefy man with hardly any neck, although he did \\\nhave a very large mustache. Mrs. Dursley was thin and blonde and had \\\nnearly twice the usual amount of neck, which came in very useful as she \\\nspent so much of her time craning over garden fences, spying on the \\\nneighbors. The Dursleys had a small son called Dudley and in their \\\nopinion there was no finer boy anywhere. \"\n\ntext = textblob.TextBlob(text)\nprint(text.sentences)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0G_T7P5744Yb","outputId":"1c2f40f9-3806-4c59-eb94-a784eda2be9d"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":"[Sentence(\"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\"), Sentence(\"They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\"), Sentence(\"Mr. Dursley was the director of a firm called Grunnings, which made drills.\"), Sentence(\"He was a big, beefy man with hardly any neck, although he did have a very large mustache.\"), Sentence(\"Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors.\"), Sentence(\"The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\")]\n"}]},{"cell_type":"markdown","source":"**Summary:** Text normalization is a standardization process. It often involves, tokenization, lemmatization, stemming and sentence segmentation.","metadata":{"id":"VKwJKODS5cwI"}},{"cell_type":"markdown","source":"#3. N-grams\n\nN-gram refers to the sequential chunks having length of n. They are pretty important in NLP applications and used in models. I find n grams similar to the sliding windows concept in time series. Let's build a custom one first.","metadata":{"id":"fW4C3t_5xSg_"}},{"cell_type":"code","source":"def create_bigrams(text):\n  bigrams = []\n  tokenized_text = re.findall(\"\\w+\",text)\n  for i in range(len(tokenized_text)-1):\n    bigrams.append((tokenized_text[i],tokenized_text[i+1]))\n\n  return bigrams","metadata":{"id":"mayjOojM6wum"},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"text = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say \\\nthat they were perfectly normal, thank you very much. They were the last \\\npeople you'd expect to be involved in anything strange or mysterious, \\\nbecause they just didn't hold with such nonsense. \\\nMr. Dursley was the director of a firm called Grunnings, which made \\\ndrills. He was a big, beefy man with hardly any neck, although he did \\\nhave a very large mustache. Mrs. Dursley was thin and blonde and had \\\nnearly twice the usual amount of neck, which came in very useful as she \\\nspent so much of her time craning over garden fences, spying on the \\\nneighbors. The Dursleys had a small son called Dudley and in their \\\nopinion there was no finer boy anywhere. \"\n\nbigrams = create_bigrams(text)\nprint(bigrams)\n","metadata":{"id":"kku1sJQMvg5A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"12301110-7ccb-4287-c63f-eac858f88d3d"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":"[('Mr', 'and'), ('and', 'Mrs'), ('Mrs', 'Dursley'), ('Dursley', 'of'), ('of', 'number'), ('number', 'four'), ('four', 'Privet'), ('Privet', 'Drive'), ('Drive', 'were'), ('were', 'proud'), ('proud', 'to'), ('to', 'say'), ('say', 'that'), ('that', 'they'), ('they', 'were'), ('were', 'perfectly'), ('perfectly', 'normal'), ('normal', 'thank'), ('thank', 'you'), ('you', 'very'), ('very', 'much'), ('much', 'They'), ('They', 'were'), ('were', 'the'), ('the', 'last'), ('last', 'people'), ('people', 'you'), ('you', 'd'), ('d', 'expect'), ('expect', 'to'), ('to', 'be'), ('be', 'involved'), ('involved', 'in'), ('in', 'anything'), ('anything', 'strange'), ('strange', 'or'), ('or', 'mysterious'), ('mysterious', 'because'), ('because', 'they'), ('they', 'just'), ('just', 'didn'), ('didn', 't'), ('t', 'hold'), ('hold', 'with'), ('with', 'such'), ('such', 'nonsense'), ('nonsense', 'Mr'), ('Mr', 'Dursley'), ('Dursley', 'was'), ('was', 'the'), ('the', 'director'), ('director', 'of'), ('of', 'a'), ('a', 'firm'), ('firm', 'called'), ('called', 'Grunnings'), ('Grunnings', 'which'), ('which', 'made'), ('made', 'drills'), ('drills', 'He'), ('He', 'was'), ('was', 'a'), ('a', 'big'), ('big', 'beefy'), ('beefy', 'man'), ('man', 'with'), ('with', 'hardly'), ('hardly', 'any'), ('any', 'neck'), ('neck', 'although'), ('although', 'he'), ('he', 'did'), ('did', 'have'), ('have', 'a'), ('a', 'very'), ('very', 'large'), ('large', 'mustache'), ('mustache', 'Mrs'), ('Mrs', 'Dursley'), ('Dursley', 'was'), ('was', 'thin'), ('thin', 'and'), ('and', 'blonde'), ('blonde', 'and'), ('and', 'had'), ('had', 'nearly'), ('nearly', 'twice'), ('twice', 'the'), ('the', 'usual'), ('usual', 'amount'), ('amount', 'of'), ('of', 'neck'), ('neck', 'which'), ('which', 'came'), ('came', 'in'), ('in', 'very'), ('very', 'useful'), ('useful', 'as'), ('as', 'she'), ('she', 'spent'), ('spent', 'so'), ('so', 'much'), ('much', 'of'), ('of', 'her'), ('her', 'time'), ('time', 'craning'), ('craning', 'over'), ('over', 'garden'), ('garden', 'fences'), ('fences', 'spying'), ('spying', 'on'), ('on', 'the'), ('the', 'neighbors'), ('neighbors', 'The'), ('The', 'Dursleys'), ('Dursleys', 'had'), ('had', 'a'), ('a', 'small'), ('small', 'son'), ('son', 'called'), ('called', 'Dudley'), ('Dudley', 'and'), ('and', 'in'), ('in', 'their'), ('their', 'opinion'), ('opinion', 'there'), ('there', 'was'), ('was', 'no'), ('no', 'finer'), ('finer', 'boy'), ('boy', 'anywhere')]\n"}]},{"cell_type":"markdown","source":"or we can extend it for the n grams.","metadata":{"id":"qXC2_TuE8SCi"}},{"cell_type":"code","source":"def create_n_grams(text,n):\n  ngrams = []\n  tokenized_text = re.findall('\\w+',text)\n  for i in range(len(tokenized_text)-n+1):\n    ngrams.append(tuple(tokenized_text[i:i+n]))\n\n  return ngrams\n","metadata":{"id":"LuWebtO88Hno"},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"text = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say \\\nthat they were perfectly normal, thank you very much. They were the last \\\npeople you'd expect to be involved in anything strange or mysterious, \\\nbecause they just didn't hold with such nonsense. \\\nMr. Dursley was the director of a firm called Grunnings, which made \\\ndrills. He was a big, beefy man with hardly any neck, although he did \\\nhave a very large mustache. Mrs. Dursley was thin and blonde and had \\\nnearly twice the usual amount of neck, which came in very useful as she \\\nspent so much of her time craning over garden fences, spying on the \\\nneighbors. The Dursleys had a small son called Dudley and in their \\\nopinion there was no finer boy anywhere. \"\n\nngrams = create_n_grams(text,3)\nprint(ngrams)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoXK-fj_8yPB","outputId":"777cb6af-7a27-43ee-de5d-041136570dab"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":"[('Mr', 'and', 'Mrs'), ('and', 'Mrs', 'Dursley'), ('Mrs', 'Dursley', 'of'), ('Dursley', 'of', 'number'), ('of', 'number', 'four'), ('number', 'four', 'Privet'), ('four', 'Privet', 'Drive'), ('Privet', 'Drive', 'were'), ('Drive', 'were', 'proud'), ('were', 'proud', 'to'), ('proud', 'to', 'say'), ('to', 'say', 'that'), ('say', 'that', 'they'), ('that', 'they', 'were'), ('they', 'were', 'perfectly'), ('were', 'perfectly', 'normal'), ('perfectly', 'normal', 'thank'), ('normal', 'thank', 'you'), ('thank', 'you', 'very'), ('you', 'very', 'much'), ('very', 'much', 'They'), ('much', 'They', 'were'), ('They', 'were', 'the'), ('were', 'the', 'last'), ('the', 'last', 'people'), ('last', 'people', 'you'), ('people', 'you', 'd'), ('you', 'd', 'expect'), ('d', 'expect', 'to'), ('expect', 'to', 'be'), ('to', 'be', 'involved'), ('be', 'involved', 'in'), ('involved', 'in', 'anything'), ('in', 'anything', 'strange'), ('anything', 'strange', 'or'), ('strange', 'or', 'mysterious'), ('or', 'mysterious', 'because'), ('mysterious', 'because', 'they'), ('because', 'they', 'just'), ('they', 'just', 'didn'), ('just', 'didn', 't'), ('didn', 't', 'hold'), ('t', 'hold', 'with'), ('hold', 'with', 'such'), ('with', 'such', 'nonsense'), ('such', 'nonsense', 'Mr'), ('nonsense', 'Mr', 'Dursley'), ('Mr', 'Dursley', 'was'), ('Dursley', 'was', 'the'), ('was', 'the', 'director'), ('the', 'director', 'of'), ('director', 'of', 'a'), ('of', 'a', 'firm'), ('a', 'firm', 'called'), ('firm', 'called', 'Grunnings'), ('called', 'Grunnings', 'which'), ('Grunnings', 'which', 'made'), ('which', 'made', 'drills'), ('made', 'drills', 'He'), ('drills', 'He', 'was'), ('He', 'was', 'a'), ('was', 'a', 'big'), ('a', 'big', 'beefy'), ('big', 'beefy', 'man'), ('beefy', 'man', 'with'), ('man', 'with', 'hardly'), ('with', 'hardly', 'any'), ('hardly', 'any', 'neck'), ('any', 'neck', 'although'), ('neck', 'although', 'he'), ('although', 'he', 'did'), ('he', 'did', 'have'), ('did', 'have', 'a'), ('have', 'a', 'very'), ('a', 'very', 'large'), ('very', 'large', 'mustache'), ('large', 'mustache', 'Mrs'), ('mustache', 'Mrs', 'Dursley'), ('Mrs', 'Dursley', 'was'), ('Dursley', 'was', 'thin'), ('was', 'thin', 'and'), ('thin', 'and', 'blonde'), ('and', 'blonde', 'and'), ('blonde', 'and', 'had'), ('and', 'had', 'nearly'), ('had', 'nearly', 'twice'), ('nearly', 'twice', 'the'), ('twice', 'the', 'usual'), ('the', 'usual', 'amount'), ('usual', 'amount', 'of'), ('amount', 'of', 'neck'), ('of', 'neck', 'which'), ('neck', 'which', 'came'), ('which', 'came', 'in'), ('came', 'in', 'very'), ('in', 'very', 'useful'), ('very', 'useful', 'as'), ('useful', 'as', 'she'), ('as', 'she', 'spent'), ('she', 'spent', 'so'), ('spent', 'so', 'much'), ('so', 'much', 'of'), ('much', 'of', 'her'), ('of', 'her', 'time'), ('her', 'time', 'craning'), ('time', 'craning', 'over'), ('craning', 'over', 'garden'), ('over', 'garden', 'fences'), ('garden', 'fences', 'spying'), ('fences', 'spying', 'on'), ('spying', 'on', 'the'), ('on', 'the', 'neighbors'), ('the', 'neighbors', 'The'), ('neighbors', 'The', 'Dursleys'), ('The', 'Dursleys', 'had'), ('Dursleys', 'had', 'a'), ('had', 'a', 'small'), ('a', 'small', 'son'), ('small', 'son', 'called'), ('son', 'called', 'Dudley'), ('called', 'Dudley', 'and'), ('Dudley', 'and', 'in'), ('and', 'in', 'their'), ('in', 'their', 'opinion'), ('their', 'opinion', 'there'), ('opinion', 'there', 'was'), ('there', 'was', 'no'), ('was', 'no', 'finer'), ('no', 'finer', 'boy'), ('finer', 'boy', 'anywhere')]\n"}]},{"cell_type":"markdown","source":"Here is an easier way to do:","metadata":{"id":"dVH6xdvO9lNe"}},{"cell_type":"code","source":"text = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say \\\nthat they were perfectly normal, thank you very much. They were the last \\\npeople you'd expect to be involved in anything strange or mysterious, \\\nbecause they just didn't hold with such nonsense. \\\nMr. Dursley was the director of a firm called Grunnings, which made \\\ndrills. He was a big, beefy man with hardly any neck, although he did \\\nhave a very large mustache. Mrs. Dursley was thin and blonde and had \\\nnearly twice the usual amount of neck, which came in very useful as she \\\nspent so much of her time craning over garden fences, spying on the \\\nneighbors. The Dursleys had a small son called Dudley and in their \\\nopinion there was no finer boy anywhere. \"\n\ntext = textblob.TextBlob(text)\nprint(text.ngrams(n = 2))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"30fRvfHm87CP","outputId":"886a5443-9c2b-44bf-c688-7b0a4fd1bb9d"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":"[WordList(['Mr', 'and']), WordList(['and', 'Mrs']), WordList(['Mrs', 'Dursley']), WordList(['Dursley', 'of']), WordList(['of', 'number']), WordList(['number', 'four']), WordList(['four', 'Privet']), WordList(['Privet', 'Drive']), WordList(['Drive', 'were']), WordList(['were', 'proud']), WordList(['proud', 'to']), WordList(['to', 'say']), WordList(['say', 'that']), WordList(['that', 'they']), WordList(['they', 'were']), WordList(['were', 'perfectly']), WordList(['perfectly', 'normal']), WordList(['normal', 'thank']), WordList(['thank', 'you']), WordList(['you', 'very']), WordList(['very', 'much']), WordList(['much', 'They']), WordList(['They', 'were']), WordList(['were', 'the']), WordList(['the', 'last']), WordList(['last', 'people']), WordList(['people', 'you']), WordList(['you', \"'d\"]), WordList([\"'d\", 'expect']), WordList(['expect', 'to']), WordList(['to', 'be']), WordList(['be', 'involved']), WordList(['involved', 'in']), WordList(['in', 'anything']), WordList(['anything', 'strange']), WordList(['strange', 'or']), WordList(['or', 'mysterious']), WordList(['mysterious', 'because']), WordList(['because', 'they']), WordList(['they', 'just']), WordList(['just', 'did']), WordList(['did', \"n't\"]), WordList([\"n't\", 'hold']), WordList(['hold', 'with']), WordList(['with', 'such']), WordList(['such', 'nonsense']), WordList(['nonsense', 'Mr']), WordList(['Mr', 'Dursley']), WordList(['Dursley', 'was']), WordList(['was', 'the']), WordList(['the', 'director']), WordList(['director', 'of']), WordList(['of', 'a']), WordList(['a', 'firm']), WordList(['firm', 'called']), WordList(['called', 'Grunnings']), WordList(['Grunnings', 'which']), WordList(['which', 'made']), WordList(['made', 'drills']), WordList(['drills', 'He']), WordList(['He', 'was']), WordList(['was', 'a']), WordList(['a', 'big']), WordList(['big', 'beefy']), WordList(['beefy', 'man']), WordList(['man', 'with']), WordList(['with', 'hardly']), WordList(['hardly', 'any']), WordList(['any', 'neck']), WordList(['neck', 'although']), WordList(['although', 'he']), WordList(['he', 'did']), WordList(['did', 'have']), WordList(['have', 'a']), WordList(['a', 'very']), WordList(['very', 'large']), WordList(['large', 'mustache']), WordList(['mustache', 'Mrs']), WordList(['Mrs', 'Dursley']), WordList(['Dursley', 'was']), WordList(['was', 'thin']), WordList(['thin', 'and']), WordList(['and', 'blonde']), WordList(['blonde', 'and']), WordList(['and', 'had']), WordList(['had', 'nearly']), WordList(['nearly', 'twice']), WordList(['twice', 'the']), WordList(['the', 'usual']), WordList(['usual', 'amount']), WordList(['amount', 'of']), WordList(['of', 'neck']), WordList(['neck', 'which']), WordList(['which', 'came']), WordList(['came', 'in']), WordList(['in', 'very']), WordList(['very', 'useful']), WordList(['useful', 'as']), WordList(['as', 'she']), WordList(['she', 'spent']), WordList(['spent', 'so']), WordList(['so', 'much']), WordList(['much', 'of']), WordList(['of', 'her']), WordList(['her', 'time']), WordList(['time', 'craning']), WordList(['craning', 'over']), WordList(['over', 'garden']), WordList(['garden', 'fences']), WordList(['fences', 'spying']), WordList(['spying', 'on']), WordList(['on', 'the']), WordList(['the', 'neighbors']), WordList(['neighbors', 'The']), WordList(['The', 'Dursleys']), WordList(['Dursleys', 'had']), WordList(['had', 'a']), WordList(['a', 'small']), WordList(['small', 'son']), WordList(['son', 'called']), WordList(['called', 'Dudley']), WordList(['Dudley', 'and']), WordList(['and', 'in']), WordList(['in', 'their']), WordList(['their', 'opinion']), WordList(['opinion', 'there']), WordList(['there', 'was']), WordList(['was', 'no']), WordList(['no', 'finer']), WordList(['finer', 'boy']), WordList(['boy', 'anywhere'])]\n"}]}]}