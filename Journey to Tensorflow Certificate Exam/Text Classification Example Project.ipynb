{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EkomfiZEygF-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_dataset,test_dataset),ds_info = tfds.load('imdb_reviews',\n",
        "                                                 with_info = True,\n",
        "                                         split = ['train','test'],\n",
        "                                                  as_supervised = True,\n",
        "                                         shuffle_files = True,\n",
        "                                         batch_size = 128\n",
        "\n",
        "                                                 )"
      ],
      "metadata": {
        "id": "TEs_qt6JzqPc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_vectorization(dataset,vectorizer):\n",
        "  \"\"\"\n",
        "  Uses TextVectorization Layer outside of the model.\n",
        "  The purpose is to make the NN model suitable to .h5 format.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset: Tensorflow dataset containing data and labels.\n",
        "  vectorizer: A TextVectorization layer adapted to train_dataset\n",
        "  \"\"\"\n",
        "  #conduct all the process in CPU to CPU-GPU conflict\n",
        "  with tf.device(\"/cpu:0\"):\n",
        "    outputs = []\n",
        "    label_list = []\n",
        "\n",
        "    #tokenize and pad the data and store it\n",
        "    for x, y in dataset:\n",
        "        output = vectorizer(x)\n",
        "        outputs.append(output)\n",
        "        label_list.append(y)\n",
        "\n",
        "    #concatenate the labels and data\n",
        "    X_vectorized = tf.concat(outputs, axis=0)\n",
        "    Y_labels = tf.concat(label_list, axis=0)\n",
        "\n",
        "    vectorized = tf.data.Dataset.from_tensor_slices((X_vectorized, Y_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "    return vectorized\n",
        "\n",
        "def define_callbacks(model):\n",
        "  es = tf.keras.callbacks.EarlyStopping(patience = 5,verbose = 1, restore_best_weights = True)\n",
        "  mc = tf.keras.callbacks.ModelCheckpoint(filepath = f\"./ModelCheckpoints/{model.name}.ckpt\",\n",
        "                                         save_best_only = True,\n",
        "                                         save_weights_only = True)\n",
        "  tb = tf.keras.callbacks.TensorBoard(log_dir = f\"./TensorboardLogs/{model.name}\")\n",
        "\n",
        "  return es,mc,tb"
      ],
      "metadata": {
        "id": "KOTT_BitYSjt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the vocab length\n",
        "vectorizer = layers.TextVectorization(max_tokens = None)\n",
        "vectorizer.adapt(train_dataset.map(lambda x,y: x))\n",
        "total_words = vectorizer.vocabulary_size()"
      ],
      "metadata": {
        "id": "L9-ou25l22aP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the length of the sequences to find the output sequence length\n",
        "text_lengths = []\n",
        "train_dataset_unbatched = train_dataset.unbatch()\n",
        "for text,label in train_dataset_unbatched: #unbatch the dataset\n",
        "  text_lengths.append(len(text.numpy().split())) #append the sentence length"
      ],
      "metadata": {
        "id": "ObkFjEj96HBb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create model and history dictionaries\n",
        "models = {}\n",
        "histories = {}"
      ],
      "metadata": {
        "id": "ayhlFuIpiNWX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 20000\n",
        "OUTPUT_SEQUENCE_LENGTH = int(tfp.stats.percentile(text_lengths,95).numpy())\n",
        "EMBEDDING_DIM = 128\n",
        "MODEL_NAME = \"model1\"\n",
        "HISTORY_NAME = 'history1'\n",
        "\n",
        "#define the actual vectorizer\n",
        "vectorizer = tf.keras.layers.TextVectorization(max_tokens = MAX_TOKENS,\n",
        "                                               output_sequence_length = OUTPUT_SEQUENCE_LENGTH)\n",
        "\n",
        "vectorizer.adapt(train_dataset.map(lambda x,y: x))\n",
        "#preprocess the data\n",
        "train_dataset_vectorized1 = text_vectorization(train_dataset,vectorizer)\n",
        "test_dataset_vectorized1 = text_vectorization(test_dataset,vectorizer)\n",
        "\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape = (598,))\n",
        "\n",
        "x = layers.Embedding(input_dim = MAX_TOKENS,\n",
        "                      output_dim = EMBEDDING_DIM)(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
        "\n",
        "#create the base model\n",
        "models[MODEL_NAME] = tf.keras.Model(inputs,outputs,name = MODEL_NAME)\n",
        "\n",
        "\n",
        "#compile the model\n",
        "models[MODEL_NAME].compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "#fit the model\n",
        "histories[HISTORY_NAME] = models[MODEL_NAME].fit(train_dataset_vectorized1,validation_data = test_dataset_vectorized1,epochs = 5,\n",
        "                    )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOE1KzPA6kRB",
        "outputId": "13af309c-3752-42f2-df98-fa464ac31e0b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 109s 137ms/step - loss: 0.6057 - accuracy: 0.7140 - val_loss: 0.4894 - val_accuracy: 0.8165\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.3901 - accuracy: 0.8631 - val_loss: 0.3611 - val_accuracy: 0.8650\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2939 - accuracy: 0.8950 - val_loss: 0.3164 - val_accuracy: 0.8802\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2440 - accuracy: 0.9125 - val_loss: 0.2961 - val_accuracy: 0.8854\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2096 - accuracy: 0.9267 - val_loss: 0.2866 - val_accuracy: 0.8872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add another Dense Layer -> Not worked. Increased overfitting."
      ],
      "metadata": {
        "id": "Z6vXq93-kstX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 20000\n",
        "OUTPUT_SEQUENCE_LENGTH = int(tfp.stats.percentile(text_lengths,95).numpy())\n",
        "EMBEDDING_DIM = 128\n",
        "MODEL_NAME = \"model2\"\n",
        "HISTORY_NAME = 'history2'\n",
        "\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape = (598,))\n",
        "\n",
        "x = layers.Embedding(input_dim = MAX_TOKENS,\n",
        "                      output_dim = EMBEDDING_DIM)(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dense(64,activation = 'relu')(x)\n",
        "outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
        "\n",
        "#create the base model\n",
        "models[MODEL_NAME] = tf.keras.Model(inputs,outputs,name = MODEL_NAME)\n",
        "\n",
        "#compile the model\n",
        "models[MODEL_NAME].compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "#fit the model\n",
        "histories[HISTORY_NAME] = models[MODEL_NAME].fit(train_dataset_vectorized1,validation_data = test_dataset_vectorized1,epochs = 5,\n",
        "                   )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToZQ6pXg9QxO",
        "outputId": "5a977c86-71e8-4741-da45-e6090d1a7357"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 78s 99ms/step - loss: 0.4440 - accuracy: 0.7880 - val_loss: 0.3000 - val_accuracy: 0.8807\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2212 - accuracy: 0.9142 - val_loss: 0.2848 - val_accuracy: 0.8849\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.1579 - accuracy: 0.9423 - val_loss: 0.2976 - val_accuracy: 0.8850\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1180 - accuracy: 0.9589 - val_loss: 0.3413 - val_accuracy: 0.8786\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.0927 - accuracy: 0.9695 - val_loss: 0.3972 - val_accuracy: 0.8713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decrese the tokens -> Worked. Overfitting decreased."
      ],
      "metadata": {
        "id": "Q8EkRCdXk33D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 10000\n",
        "OUTPUT_SEQUENCE_LENGTH = int(tfp.stats.percentile(text_lengths,95).numpy())\n",
        "EMBEDDING_DIM = 50\n",
        "MODEL_NAME = 'model3'\n",
        "HISTORY_NAME = 'history3'\n",
        "\n",
        "#define the actual vectorizer\n",
        "vectorizer = tf.keras.layers.TextVectorization(max_tokens = MAX_TOKENS,\n",
        "                                               output_sequence_length = OUTPUT_SEQUENCE_LENGTH)\n",
        "vectorizer.adapt(train_dataset.map(lambda x,y: x))\n",
        "\n",
        "#preprocess the data\n",
        "train_dataset_vectorized2 = text_vectorization(train_dataset,vectorizer)\n",
        "test_dataset_vectorized2 = text_vectorization(test_dataset,vectorizer)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape = (598,))\n",
        "x = layers.Embedding(input_dim = MAX_TOKENS,\n",
        "                      output_dim = EMBEDDING_DIM)(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "#x = layers.Dense(64,activation = 'relu')(x)\n",
        "outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
        "\n",
        "#create the base model\n",
        "models[MODEL_NAME] = tf.keras.Model(inputs,outputs,name = MODEL_NAME)\n",
        "\n",
        "\n",
        "#compile the model\n",
        "models[MODEL_NAME].compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "#fit the model\n",
        "histories[HISTORY_NAME] = models[MODEL_NAME].fit(train_dataset_vectorized2,validation_data = test_dataset_vectorized2,epochs = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgMbFh9BBKSf",
        "outputId": "68139da6-44f4-4fa7-ddcf-719fb3192219"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 113s 144ms/step - loss: 0.6419 - accuracy: 0.6891 - val_loss: 0.5628 - val_accuracy: 0.7730\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4719 - accuracy: 0.8341 - val_loss: 0.4248 - val_accuracy: 0.8457\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3641 - accuracy: 0.8722 - val_loss: 0.3602 - val_accuracy: 0.8653\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3085 - accuracy: 0.8880 - val_loss: 0.3275 - val_accuracy: 0.8748\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2737 - accuracy: 0.9000 - val_loss: 0.3086 - val_accuracy: 0.8810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try Flattening. Not worked Increased overfitting a lot."
      ],
      "metadata": {
        "id": "eDGGrVeRk98C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 10000\n",
        "OUTPUT_SEQUENCE_LENGTH = int(tfp.stats.percentile(text_lengths,95).numpy())\n",
        "EMBEDDING_DIM = 50\n",
        "MODEL_NAME = 'model4'\n",
        "HISTORY_NAME = 'history4'\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape = (598,))\n",
        "x = layers.Embedding(input_dim = MAX_TOKENS,\n",
        "                      output_dim = EMBEDDING_DIM)(inputs)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
        "\n",
        "#create the base model\n",
        "models[MODEL_NAME] = tf.keras.Model(inputs,outputs,name = MODEL_NAME)\n",
        "\n",
        "#create the callbacks\n",
        "es,mc,tb = define_callbacks(models[MODEL_NAME])\n",
        "\n",
        "#compile the model\n",
        "models[MODEL_NAME].compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "#fit the model\n",
        "histories[HISTORY_NAME] = models[MODEL_NAME].fit(train_dataset_vectorized2,validation_data = test_dataset_vectorized2,epochs = 5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdKSSHjpCO_F",
        "outputId": "687f635e-bbc2-4403-ad3b-84c8cbfd4fc3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 78s 99ms/step - loss: 0.4349 - accuracy: 0.7898 - val_loss: 0.3087 - val_accuracy: 0.8706\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.1890 - accuracy: 0.9312 - val_loss: 0.3167 - val_accuracy: 0.8681\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0875 - accuracy: 0.9773 - val_loss: 0.3518 - val_accuracy: 0.8628\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0337 - accuracy: 0.9960 - val_loss: 0.3921 - val_accuracy: 0.8612\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.0133 - accuracy: 0.9992 - val_loss: 0.4283 - val_accuracy: 0.8604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model 3 for longer."
      ],
      "metadata": {
        "id": "i5YBGh7Il9Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 10000\n",
        "OUTPUT_SEQUENCE_LENGTH = int(tfp.stats.percentile(text_lengths,95).numpy())\n",
        "EMBEDDING_DIM = 50\n",
        "MODEL_NAME = 'model5'\n",
        "HISTORY_NAME = 'history5'\n",
        "\n",
        "#define the actual vectorizer\n",
        "vectorizer = tf.keras.layers.TextVectorization(max_tokens = MAX_TOKENS,\n",
        "                                               output_sequence_length = OUTPUT_SEQUENCE_LENGTH)\n",
        "vectorizer.adapt(train_dataset.map(lambda x,y: x))\n",
        "\n",
        "#preprocess the data\n",
        "train_dataset_vectorized2 = text_vectorization(train_dataset,vectorizer)\n",
        "test_dataset_vectorized2 = text_vectorization(test_dataset,vectorizer)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape = (598,))\n",
        "x = layers.Embedding(input_dim = MAX_TOKENS,\n",
        "                      output_dim = EMBEDDING_DIM)(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "#x = layers.Dense(64,activation = 'relu')(x)\n",
        "outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
        "\n",
        "#create the base model\n",
        "models[MODEL_NAME] = tf.keras.Model(inputs,outputs,name = MODEL_NAME)\n",
        "\n",
        "#define callbacks\n",
        "es,mc,tb = define_callbacks(models[MODEL_NAME])\n",
        "\n",
        "#compile the model\n",
        "models[MODEL_NAME].compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "#fit the model\n",
        "histories[HISTORY_NAME] = models[MODEL_NAME].fit(train_dataset_vectorized2,validation_data = test_dataset_vectorized2,epochs = 100,\n",
        "                                                 callbacks = [es,mc,tb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBajVyWbl5w8",
        "outputId": "228a09bb-523f-4726-99fe-ebbc2eaa0442"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "782/782 [==============================] - 80s 100ms/step - loss: 0.6396 - accuracy: 0.6891 - val_loss: 0.5574 - val_accuracy: 0.7766\n",
            "Epoch 2/100\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.4664 - accuracy: 0.8356 - val_loss: 0.4203 - val_accuracy: 0.8472\n",
            "Epoch 3/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3601 - accuracy: 0.8730 - val_loss: 0.3573 - val_accuracy: 0.8666\n",
            "Epoch 4/100\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.3055 - accuracy: 0.8882 - val_loss: 0.3255 - val_accuracy: 0.8758\n",
            "Epoch 5/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2711 - accuracy: 0.9010 - val_loss: 0.3071 - val_accuracy: 0.8812\n",
            "Epoch 6/100\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2460 - accuracy: 0.9104 - val_loss: 0.2959 - val_accuracy: 0.8837\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2260 - accuracy: 0.9179 - val_loss: 0.2894 - val_accuracy: 0.8856\n",
            "Epoch 8/100\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2094 - accuracy: 0.9242 - val_loss: 0.2861 - val_accuracy: 0.8871\n",
            "Epoch 9/100\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.1951 - accuracy: 0.9296 - val_loss: 0.2853 - val_accuracy: 0.8875\n",
            "Epoch 10/100\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.1827 - accuracy: 0.9345 - val_loss: 0.2864 - val_accuracy: 0.8872\n",
            "Epoch 11/100\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.1716 - accuracy: 0.9389 - val_loss: 0.2892 - val_accuracy: 0.8863\n",
            "Epoch 12/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.1617 - accuracy: 0.9434 - val_loss: 0.2932 - val_accuracy: 0.8845\n",
            "Epoch 13/100\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.1526 - accuracy: 0.9471 - val_loss: 0.2983 - val_accuracy: 0.8840\n",
            "Epoch 14/100\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.1443 - accuracy: 0.9503Restoring model weights from the end of the best epoch: 9.\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.1443 - accuracy: 0.9502 - val_loss: 0.3044 - val_accuracy: 0.8836\n",
            "Epoch 14: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add dropout and Dense layer. Change Activation to swish"
      ],
      "metadata": {
        "id": "vh64GoYYmpEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 10000\n",
        "OUTPUT_SEQUENCE_LENGTH = int(tfp.stats.percentile(text_lengths,95).numpy())\n",
        "EMBEDDING_DIM = 50\n",
        "MODEL_NAME = 'model6'\n",
        "HISTORY_NAME = 'history6'\n",
        "ACTIVATION = 'swish'\n",
        "\n",
        "#define the actual vectorizer\n",
        "vectorizer = tf.keras.layers.TextVectorization(max_tokens = MAX_TOKENS,\n",
        "                                               output_sequence_length = OUTPUT_SEQUENCE_LENGTH)\n",
        "vectorizer.adapt(train_dataset.map(lambda x,y: x))\n",
        "\n",
        "#preprocess the data\n",
        "train_dataset_vectorized2 = text_vectorization(train_dataset,vectorizer)\n",
        "test_dataset_vectorized2 = text_vectorization(test_dataset,vectorizer)\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape = (598,))\n",
        "x = layers.Embedding(input_dim = MAX_TOKENS,\n",
        "                      output_dim = EMBEDDING_DIM)(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dense(64,activation = ACTIVATION)(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
        "\n",
        "#create the base model\n",
        "models[MODEL_NAME] = tf.keras.Model(inputs,outputs,name = MODEL_NAME)\n",
        "\n",
        "#define callbacks\n",
        "es,mc,tb = define_callbacks(models[MODEL_NAME])\n",
        "\n",
        "#compile the model\n",
        "models[MODEL_NAME].compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "#fit the model\n",
        "histories[HISTORY_NAME] = models[MODEL_NAME].fit(train_dataset_vectorized2,validation_data = test_dataset_vectorized2,epochs = 100,\n",
        "                                                 callbacks = [es,mc,tb])"
      ],
      "metadata": {
        "id": "5mGOUyIWmll8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a54bb9-6302-4be3-880d-0616551660ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "782/782 [==============================] - 78s 98ms/step - loss: 0.5004 - accuracy: 0.7428 - val_loss: 0.3224 - val_accuracy: 0.8728\n",
            "Epoch 2/100\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2618 - accuracy: 0.8980 - val_loss: 0.2858 - val_accuracy: 0.8878\n",
            "Epoch 3/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2070 - accuracy: 0.9220 - val_loss: 0.2886 - val_accuracy: 0.8873\n",
            "Epoch 4/100\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1736 - accuracy: 0.9356 - val_loss: 0.3090 - val_accuracy: 0.8835\n",
            "Epoch 5/100\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1495 - accuracy: 0.9444 - val_loss: 0.3419 - val_accuracy: 0.8774\n",
            "Epoch 6/100\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1292 - accuracy: 0.9544 - val_loss: 0.3807 - val_accuracy: 0.8734\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.9614Restoring model weights from the end of the best epoch: 2.\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1122 - accuracy: 0.9614 - val_loss: 0.4261 - val_accuracy: 0.8703\n",
            "Epoch 7: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: LSTM does not work in GPU when it is used with relu;"
      ],
      "metadata": {
        "id": "zkpATTD_YdeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!tensorboard dev upload --logdir /content/TensorboardLogs"
      ],
      "metadata": {
        "id": "ZqPuHA_QGsCn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clone the model\n",
        "final_model = tf.keras.models.clone_model(models['model5'])\n",
        "\n",
        "#load the best weights\n",
        "final_model.load_weights(\"/content/ModelCheckpoints/model5.ckpt\")\n",
        "\n",
        "#compile the model\n",
        "final_model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "#evaluate the test dataset\n",
        "final_model.evaluate(test_dataset_vectorized2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSyLBK9AJN7M",
        "outputId": "e56c5ca5-e88f-4968-cedb-013232d3f7eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2853 - accuracy: 0.8875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28529679775238037, 0.8875200152397156]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!tensorboard dev list"
      ],
      "metadata": {
        "id": "laOVVWpxJyoG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!tensorboard dev delete --experiment_id rvOlSmxeQMiBvT4XhMmLdw"
      ],
      "metadata": {
        "id": "FEuiqq62J5D6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final_model.save('final_model.h5')"
      ],
      "metadata": {
        "id": "UgiHRxC6BbAV"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}