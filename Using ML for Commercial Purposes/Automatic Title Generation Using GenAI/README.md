# Project Summary

The purpose of this project is to leverage Generative AI to produce content that can be used in customer services. This is especially applicable if these models are fine tuned with the more spesific data.


In this work, I tried to improve  title generation capabilities of Gemini Flash 1.5 by training the model with the titles that had been successful before. The success is dependent to the context so don't bother with how these titles were selected. In fact, to have a set of succesfull titles, I used another LLM (Microsoft Phi 3.5).

First, I requested Gemini and Microsoft Phi to generate titles about cars that can potentially improve the sales (`gemini_base_response` and `microsoft_titles` respectively). Then I "slightly" fine tuned Gemini with the titles generated by Microsoft Phi. I'm emphasizing the word *slightly* because 

1. If the model trains more, it generates almost the same titles which has no value.

2. Fine tuning can make the performance of the LLM's worse.

After fine tuning, I requested Gemini to generate titles about cars one more time. Lastly, for the context evaluation I used a Sentence Transformer (all-MiniLM-L6-v2). This allowed me to evaluate the [*context similarity*](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2#:~:text=Our%20model%20is%20intended%20to%20be%20used%20as%20a%20sentence%20and%20short%20paragraph%20encoder.%20Given%20an%20input%20text%2C%20it%20outputs%20a%20vector%20which%20captures%20the%20semantic%20information.%20The%20sentence%20vector%20may%20be%20used%20for%20information%20retrieval%2C%20clustering%20or%20sentence%20similarity%20tasks.) of the titles by turning each title to the context embeddings. Since Gemini generated a handful of titles in each time, I used mean pooling for comparison and I used cosine similarity as the evaluation metric.

# Results

Fine tuned Gemini  


