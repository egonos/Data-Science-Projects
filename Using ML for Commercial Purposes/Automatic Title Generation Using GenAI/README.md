# Project Summary

The purpose of this project is to leverage Generative AI to produce content that can be used in customer services. This is especially applicable if these models are fine tuned with more specific data.


In this work, I tried to improve  title generation capabilities of Gemini Flash 1.5 by training the model with the titles that were previously successful. The success is dependent on the context so the method for selecting these titles is not the focus here. In fact, to have a set of succesfull titles, I used another LLM (Microsoft Phi 3.5).

First, I requested Gemini and Microsoft Phi to generate titles about cars that can potentially improve the sales (`gemini_base_response` and `microsoft_titles` respectively). Then I "slightly" fine tuned Gemini with the titles generated by Microsoft Phi. I emphasize the word slightly because of the following reasons:

1. If the model trains more, it generates almost the same titles which has no value.

2. Fine tuning might make the performance of the LLM's worse.

After fine tuning, I requested Gemini to generate titles about cars again. Lastly, to evaluate the context I used a Sentence Transformer (all-MiniLM-L6-v2). This allowed me to evaluate the [*context similarity*](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2#:~:text=Our%20model%20is%20intended%20to%20be%20used%20as%20a%20sentence%20and%20short%20paragraph%20encoder.%20Given%20an%20input%20text%2C%20it%20outputs%20a%20vector%20which%20captures%20the%20semantic%20information.%20The%20sentence%20vector%20may%20be%20used%20for%20information%20retrieval%2C%20clustering%20or%20sentence%20similarity%20tasks.) of the titles by turning each title to the context embeddings. Since Gemini generated a handful of titles in each time, I used mean pooling for comparison and used cosine similarity as the evaluation metric.

# Results

Fine tuned Gemini produced more similar titles to the Microsoft Phi compared to the untrained Gemini model (0.76 vs 0.88). It used more emojis and columns (:) and created longer titles.


