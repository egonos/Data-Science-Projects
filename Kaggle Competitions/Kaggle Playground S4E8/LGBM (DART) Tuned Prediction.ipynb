{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7c49db",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-22T09:04:07.608717Z",
     "iopub.status.busy": "2024-11-22T09:04:07.608339Z",
     "iopub.status.idle": "2024-11-22T09:04:11.388703Z",
     "shell.execute_reply": "2024-11-22T09:04:11.387540Z"
    },
    "papermill": {
     "duration": 3.786978,
     "end_time": "2024-11-22T09:04:11.391194",
     "exception": false,
     "start_time": "2024-11-22T09:04:07.604216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install optuna-integration[lightgbm]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping,log_evaluation\n",
    "# import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd42eacd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T09:04:11.397539Z",
     "iopub.status.busy": "2024-11-22T09:04:11.397020Z",
     "iopub.status.idle": "2024-11-22T09:04:29.457877Z",
     "shell.execute_reply": "2024-11-22T09:04:29.456673Z"
    },
    "papermill": {
     "duration": 18.066794,
     "end_time": "2024-11-22T09:04:29.460559",
     "exception": false,
     "start_time": "2024-11-22T09:04:11.393765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/playground-series-s4e8/train.csv\",index_col = \"id\").sample(frac = 0.1).reset_index(drop = True) #time constraint\n",
    "test = pd.read_csv(\"/kaggle/input/playground-series-s4e8/test.csv\",index_col = \"id\").sample(frac = 0.1).reset_index(drop = True) #time constraint\n",
    "\n",
    "X = train.drop(\"class\",axis = 1)\n",
    "y = train[\"class\"]\n",
    "\n",
    "#------\n",
    "#automatic optimization needs labels to be encoded\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = pd.Series(label_encoder.fit_transform(y))\n",
    "#-----\n",
    "\n",
    "cat_cols = list(X.select_dtypes(include = \"object\").columns)\n",
    "numeric_cols = list(X.select_dtypes(include = \"float64\").columns)\n",
    "\n",
    "num_imputer = SimpleImputer(strategy = \"median\")\n",
    "cat_imputer = SimpleImputer(strategy = \"constant\",fill_value=\"None\")\n",
    "\n",
    "X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
    "\n",
    "test[numeric_cols] = num_imputer.transform(test[numeric_cols])\n",
    "test[cat_cols] = cat_imputer.transform(test[cat_cols])\n",
    "\n",
    "X[cat_cols] = X[cat_cols].astype(\"category\")\n",
    "test[cat_cols] = test[cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f864e9",
   "metadata": {
    "papermill": {
     "duration": 0.002058,
     "end_time": "2024-11-22T09:04:29.465294",
     "exception": false,
     "start_time": "2024-11-22T09:04:29.463236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "sample_size = 10000 \n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    dart_params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"boosting_type\": \"dart\",\n",
    "        \"verbosity\": -1,\n",
    "        \"random_state\": 42,\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 0.8),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 100),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)\n",
    "    }\n",
    "\n",
    "\n",
    "    X_sample, _, y_sample, _ = train_test_split(X, y, train_size=sample_size, stratify=y)\n",
    "\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    scores = []\n",
    "    \n",
    "    for train_index, valid_index in kf.split(X_sample, y_sample):\n",
    "        X_train, X_valid = X_sample.iloc[train_index], X_sample.iloc[valid_index]\n",
    "        y_train, y_valid = y_sample.iloc[train_index], y_sample.iloc[valid_index]\n",
    "        \n",
    "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "        dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "        \n",
    "        dart_model = lgb.train(dart_params,\n",
    "                               train_set=dtrain,\n",
    "                               valid_sets=[dvalid],\n",
    "                               num_boost_round=500,\n",
    "                               callbacks=[lgb.callback.log_evaluation(100)])\n",
    "        \n",
    "        preds = dart_model.predict(X_valid)\n",
    "        preds = np.round(preds).astype(int)\n",
    "        score = matthews_corrcoef(y_valid, preds)\n",
    "        scores.append(score)\n",
    "    \n",
    "  \n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c299f81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T09:04:29.471174Z",
     "iopub.status.busy": "2024-11-22T09:04:29.470760Z",
     "iopub.status.idle": "2024-11-22T09:19:22.024369Z",
     "shell.execute_reply": "2024-11-22T09:19:22.023366Z"
    },
    "papermill": {
     "duration": 892.559274,
     "end_time": "2024-11-22T09:19:22.026696",
     "exception": false,
     "start_time": "2024-11-22T09:04:29.467422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 136577, number of negative: 112778\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 942\n",
      "[LightGBM] [Info] Number of data points in the train set: 249355, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.547721 -> initscore=0.191467\n",
      "[LightGBM] [Info] Start training from score 0.191467\n",
      "[100]\tvalid_0's binary_logloss: 0.062445\n",
      "[200]\tvalid_0's binary_logloss: 0.0446146\n",
      "[300]\tvalid_0's binary_logloss: 0.0407489\n",
      "[400]\tvalid_0's binary_logloss: 0.0397888\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\tvalid_0's binary_logloss: 0.0396381\n",
      "LGBM Tuned Score Fold 1: 0.9835630850297004\n",
      "[LightGBM] [Info] Number of positive: 136577, number of negative: 112778\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 937\n",
      "[LightGBM] [Info] Number of data points in the train set: 249355, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.547721 -> initscore=0.191467\n",
      "[LightGBM] [Info] Start training from score 0.191467\n",
      "[100]\tvalid_0's binary_logloss: 0.0622966\n",
      "[200]\tvalid_0's binary_logloss: 0.0443828\n",
      "[300]\tvalid_0's binary_logloss: 0.0402185\n",
      "[400]\tvalid_0's binary_logloss: 0.0391742\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\tvalid_0's binary_logloss: 0.0390545\n",
      "LGBM Tuned Score Fold 2: 0.9840159159399227\n",
      "[LightGBM] [Info] Number of positive: 136577, number of negative: 112778\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 952\n",
      "[LightGBM] [Info] Number of data points in the train set: 249355, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.547721 -> initscore=0.191467\n",
      "[LightGBM] [Info] Start training from score 0.191467\n",
      "[100]\tvalid_0's binary_logloss: 0.0641802\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's binary_logloss: 0.0471134\n",
      "[300]\tvalid_0's binary_logloss: 0.0430559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\tvalid_0's binary_logloss: 0.0420492\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\tvalid_0's binary_logloss: 0.0421708\n",
      "LGBM Tuned Score Fold 3: 0.9815247788970217\n",
      "[LightGBM] [Info] Number of positive: 136576, number of negative: 112779\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 942\n",
      "[LightGBM] [Info] Number of data points in the train set: 249355, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.547717 -> initscore=0.191451\n",
      "[LightGBM] [Info] Start training from score 0.191451\n",
      "[100]\tvalid_0's binary_logloss: 0.0631135\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's binary_logloss: 0.0452535\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\tvalid_0's binary_logloss: 0.0414207\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\tvalid_0's binary_logloss: 0.0403144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\tvalid_0's binary_logloss: 0.0401438\n",
      "LGBM Tuned Score Fold 4: 0.9835592267460427\n",
      "[LightGBM] [Info] Number of positive: 136577, number of negative: 112779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 940\n",
      "[LightGBM] [Info] Number of data points in the train set: 249356, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.547719 -> initscore=0.191458\n",
      "[LightGBM] [Info] Start training from score 0.191458\n",
      "[100]\tvalid_0's binary_logloss: 0.0626407\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's binary_logloss: 0.0451858\n",
      "[300]\tvalid_0's binary_logloss: 0.0415829\n",
      "[400]\tvalid_0's binary_logloss: 0.0405917\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\tvalid_0's binary_logloss: 0.0406413\n",
      "LGBM Tuned Score Fold 5: 0.9829425692260478\n",
      "LGBM Tuned Average Score: 0.983121115167747\n"
     ]
    }
   ],
   "source": [
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS,random_state = 0, shuffle = True)\n",
    "\n",
    "\n",
    "dart_params = {\"feature_fraction\": 0.5422790925276052,\n",
    "    \"num_leaves\": 89,\n",
    "    \"lambda_l1\": 3.0085938483559657e-07,\n",
    "    \"lambda_l2\": 6.309353125820099e-05,\n",
    "    \"bagging_fraction\": 0.5663819462275271,\n",
    "    \"bagging_freq\": 6,\n",
    "    \"min_child_samples\": 42,\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "     'boosting_type': 'dart',\n",
    "              }\n",
    "\n",
    "\n",
    "\n",
    "scores = []\n",
    "test_predictions = []\n",
    "\n",
    "for fold,(train_idx,val_idx) in enumerate(skf.split(X,y)):\n",
    "    X_train,X_val = X.iloc[train_idx],X.iloc[val_idx]\n",
    "    y_train,y_val = y.iloc[train_idx],y.iloc[val_idx]\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train,label = y_train,categorical_feature = cat_cols)\n",
    "    dval = lgb.Dataset(X_val,label = y_val,categorical_feature = cat_cols)\n",
    "\n",
    "    tuned_model = lgb.train(dart_params,\n",
    "                          dtrain, valid_sets= [dval],\n",
    "                          categorical_feature = cat_cols,\n",
    "                            num_boost_round = 500,\n",
    "                          callbacks = [log_evaluation(100)]\n",
    "                          )\n",
    "\n",
    "    preds = tuned_model.predict(X_val)\n",
    "    score = matthews_corrcoef(y_val,np.round(preds))\n",
    "    scores.append(score)\n",
    "    \n",
    "    print(f\"LGBM Tuned Score Fold {fold+1}:\", score)\n",
    "\n",
    "    test_preds = np.round(tuned_model.predict(test))\n",
    "    test_predictions.append(test_preds)\n",
    "\n",
    "print(\"LGBM Tuned Average Score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1d811",
   "metadata": {
    "papermill": {
     "duration": 0.004443,
     "end_time": "2024-11-22T09:19:22.035837",
     "exception": false,
     "start_time": "2024-11-22T09:19:22.031394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9045607,
     "sourceId": 76727,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 918.017922,
   "end_time": "2024-11-22T09:19:22.863000",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-22T09:04:04.845078",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
