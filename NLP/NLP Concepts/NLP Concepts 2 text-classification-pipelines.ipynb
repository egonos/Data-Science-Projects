{"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Welcome to the second part of the NLP series. In this notebook, we will touch on the subject of text classification. Let's start with how a typical text generation pipeline looks like.\n\n# Building a Text Classification Pipeline\n\nA generic text classification pipeline contains the following steps:\n\n* Read the data and filter the irrelevant words (called stopwords) and symbols (punctuations, sometimes emojis etc.).\n\n* Tokenize the data.\n\n* Vectorize the data.\n\n* Classify the data.\n\nWhile this algorithm contain nuances in itself, it is mostly applicable for all the text classification purposes. Here is an introduction for each step. As we are coding, we will dive into the details more and more.\n\n-> **Filtering step:** For the base models the filtering step is important and can significantly improve the performance. On the other hand, more advanced models prefer to keep the stopwords because they may contian some useful information about the context. For example *like* and *not like* pair has opposite meanings. If we drop stopword *not*, the output may be suffer.\n\n-> **Vectorization step:** Vectorization can be done in several ways. Sometimes the more basic ones are more than enough and the other times we may need more advanced staff.\n\n\n-> **Classifier selection:** We have varity of models in text classification. Smaller ones offer faster convergence, low computational requirements and possibly high accuracies. The complex ones on the flip side, can handle pretty complex tasks and often come with their data preprocessors.\n\nThe strategy of a NLP engineer should always start with the small models, then increase the complexity.\n\nFor this notebook we will use the IMDB dataset and perform a text classification task in several ways.  ","metadata":{"id":"DdkvMbeYO30P"}},{"cell_type":"markdown","source":"## 1st Mini Project - TFIDF Vectorization & NB Classifier\n\nFor the first mini project we will use these tools:\n\n**Tokenization:** Our vectorization algorithm has a built-,n tokenizer so I won't define a new one.\n\n**Vectorization:** TFIDF Vectorization. Here is a brief explanation:\n\n**TfIDFVectorization:**\n\n\nAfter tokenizing the text, we need to give these words in somehow to our model. Now since the models only understand numerical inputs, we need to convert these words into a numerical form. We can use one hot encoding but if we do that, the dimensions increase exponentially resulting curse of dimensionality. We need a different approach. What about vectorizing the words such that *similar* or *relevant* ones have similar vectors? Ok that's appealing. To define vector relatedness we need a metric. What about taking dot product of two vectors? We know that as vectors get similar to each other, their dot product increases:\n\n$(1,0,2) · (1,2,0) = 1$\n\nand\n\n$(1,0,2) · (1,0,2) = 5$\n\nAnd we can assign the number of counts of each word in each document while assigning the vector dimension values. In the end, similar words come up at similar rates through the documents. So this should be working.\n\n One catch of this intuition is that the dot product (or similarity measure) rewards the bigger vectors. Therefore we need to normalize any word counts:\n\n $v ⟵ \\frac {v} {|v|}$\n\n $w ⟵ \\frac {w} {|w|}$\n\n And we get nothing but the cosine degree between these vectors:\n\n $cosθ = \\frac {v·w} {|v||w|}$\n\n Ok, we solved the problem but still the vectorization part is far from the truth. This is when TFIDF scoring system comes into play:\n\n\n Term Frequency (as you may guess from its meaning) refers to the occurence frequency of a word given the corpus (total number of words).\n\n$TF_{word_i} = \\frac {number\\,of\\,occurences\\,of\\,word\\,i} {total\\,number\\, of\\, words\\, in\\, corpus} $\n\nThe importance of TF is obvious. If a word occurs more often, then probably it is more important.\nWhile the claim behind TF is fruitful, it would be wrong to purely trust the results of a TF metric. For example, consider the word *the*. It's frequency is probably the highest in most of the datasets. Does this make it the most important word? No, in fact it has no well defined meaning. Therefore, we need another tool for making the model more robust. This tool is IDF. Lets look at the second component:\n\n$IDF_{word_i} = log(\\frac {number\\,of\\,documents(sentences)} {number\\,of\\,documents\\, containing\\, word\\,i})$\n\n\\\n\nand TF-IDF score is calculated by:\n\n\\\n\n$TFIDF = TF × IDF$\n\n\\\n\n","metadata":{"id":"Mba_nzzxVCvd"}},{"cell_type":"code","source":"#import the libraries\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport re\nimport textblob\nimport random\nimport requests\nimport io\nimport nltk\n\nnltk.download('punkt')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\n\n#seed the process\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)\n","metadata":{"id":"Ij8dJgpYJzeU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5962abd-c55b-4983-efb4-71ca3838ed0b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /root/nltk_data...\n\n[nltk_data]   Package punkt is already up-to-date!\n"}]},{"cell_type":"code","source":"#read and extract the dataset from the source\nURL = 'https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip'\nr = requests.get(URL, stream=True)\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall()\nz.close()","metadata":{"id":"5bYZ--pRJzt8"},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#read the dataset\nwith open(\"/content/SMSSpamCollection\",'r') as t:\n  lines = t.readlines()\n\n#display an instance\nprint(lines[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LZhPy8RGKFmU","outputId":"7b08591f-be31-4727-83ab-a55dbe134544"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n\n\n"}]},{"cell_type":"code","source":"#initialize a container\nlabels = []\ntexts = []\n\n#split the labels and texts\nfor line in lines:\n  line_split = line.split(\"\\t\")\n  label,text = line_split[0],line_split[1][:-2] #remove \\n\n  labels.append(label)\n  texts.append(text)\n\n#show the instances\nprint(\"Text samples:\",texts[0:3])\nprint(\"Label samples:\",labels[0:3])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8mAGfdZDKUJS","outputId":"6ae4e5e5-1378-48dd-a0b6-a8d8415157f2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"Text samples: ['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..', 'Ok lar... Joking wif u oni..', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18'\"]\n\nLabel samples: ['ham', 'ham', 'spam']\n"}]},{"cell_type":"code","source":"#create train test datasets\nX_train,X_test,y_train,y_test = train_test_split(texts,labels, random_state = SEED,stratify = labels)\n\nvectorizer = TfidfVectorizer() #has a built-in tokenizer so we don't necessarily define a tokenizer separately\nX_train_vectorized = vectorizer.fit_transform(X_train)\nX_test_vectorized = vectorizer.transform(X_test)","metadata":{"id":"hwzJuFQTL6TC"},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_train_vectorized","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPPmPClj6JW3","outputId":"14b1591b-9517-483d-a555-0514586bf2c2"},"execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":["<4180x7757 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 55444 stored elements in Compressed Sparse Row format>"]},"metadata":{}}]},{"cell_type":"markdown","source":"**Classifier selection:** Our first model will be Naive Bayes Classifier. Let's recall the logic behind the algorithm first.\n\nRecall the Bayes Theorem:\n\n$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$\n\nIf you haven't noticed the theorem is nothing but writing the same thing in two different form:\n\n$P(A|B) \\times P(B) = P(A\\cap B) = P(B|A) \\times P(A)$\n\nThe NB algorithm itself uses this theorem. It is called *naive* because it assumes that each variable is independent from the other. Mathematically speaking:\n\nObjective function $\\argmax_c P(c|d) = \\frac{P(d|c) \\times P(c)}{P(d)} \\quad c∈C$\n\nwhere\n\n* $c$ : Class c\n* $d$: Document d\n\n\nGenerally we omit the demoniator because it is same in all calculations.\n\n\nGiven the words $X_1,X_2,...,X_n$ in our corpus, we calculate the following:\n\n\\\n\n$P(d|X_1,X_2,...,X_n) ≈ {P(X_1,X_2,...,X_n|d) \\times P(d)}$\n\n\\\n\nand\n\n\\\n\n$P(X_1,X_2,...,X_n|d) ≈ P(X_1|d) \\times P(X_2|d)... \\times P(X_n|d)$ - Niave Assumption\n\n\\\n\nIf you haven't before, I highly recommend the reader to practise the algorithm on a small scale and run it manually on the paper. [Here](https://gauthamsanthosh.medium.com/understanding-naive-bayes-in-real-world-3c4da612a0cf) you can see an example for the algorithm.\n\n**One last note:** To speed up the algorithm and prevent underflow we compute these calcuations in the logaritmic scale.\n\n","metadata":{"id":"oQbe_G1YuL9p"}},{"cell_type":"code","source":"nb = MultinomialNB()\n\nnb.fit(X_train_vectorized,y_train)\n\npreds = nb.predict(X_test_vectorized)\n\nresults = classification_report(y_test,preds)","metadata":{"id":"hj4kFofqn9hu"},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(results)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JVT3FsugoqE1","outputId":"da4c0bab-10e0-4e35-cfd1-211b5851c771"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"              precision    recall  f1-score   support\n\n\n\n         ham       0.96      1.00      0.98      1207\n\n        spam       1.00      0.71      0.83       187\n\n\n\n    accuracy                           0.96      1394\n\n   macro avg       0.98      0.85      0.90      1394\n\nweighted avg       0.96      0.96      0.96      1394\n\n\n"}]},{"cell_type":"markdown","source":"## 2nd Mini Project Word2Vec and Logistic Regression\n\nFor the second project, we will change both our vectorization classification methods. Let's start with Word2Vec.\n\n**Word2Vec**\n\nOne of the biggest downsides of the TF-IDF is it does not consider the place of a word. If you heard Bag-of-Words terms before it refers this. Split the words in a phrase and put them into a pocket.\n\nWe now that *context* is important in languages. The words can have multiple meanings or words can be used figuratively. the first vectorization form including the context is Word2Vec. It is a shallow Neural Network (NN) architecture trained on n-grams and negative samples (maybe it is called differently I just meant the samples obtained from negative sampling). After training on these, our NN architecture gives us a set of vector called embeddings.\n\nDifference between embeddings and vectors:\n\nAn embedding is a subset  of term vector. They are short and usually in 50-100 dimensions.\n\n","metadata":{"id":"mqeYUUSf-No5"}},{"cell_type":"code","source":"#import the libraries\nfrom sklearn.linear_model import LogisticRegression\nimport gensim\n\n#set seed\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)\n","metadata":{"id":"sOIbImLzDrox"},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#tokenize the training instance one by one\ntokenized_training_words = [nltk.word_tokenize(text) for text in X_train]\ntokenized_test_words = [nltk.word_tokenize(text) for text in X_test]\n\n#adapt a word2vec to the training tokens\nvectorizer2 = gensim.models.Word2Vec(tokenized_training_words,min_count = 2)","metadata":{"id":"CGGwAP-vCQq6"},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Texts have variable length however logistic regression expects a constant input shape. To overcome this issue, we will get the mean of the word vectors for each document.","metadata":{"id":"BnFXeIM_Wche"}},{"cell_type":"code","source":"def get_mean_vector(word2vec_model, tokens):\n\n    #if the token is in the vocab then go ahead and return me the embedding vector\n    vector_list = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv.key_to_index]\n\n    #if any of the tokens are not in the corpus return 0\n    if len(vector_list) == 0:\n        return np.zeros(word2vec_model.vector_size)\n\n    #otherwise return the mean of the vector list\n    return np.mean(vector_list, axis=0)","metadata":{"id":"YjDbNH92Q__K"},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#get the word2vec vector\ntrain_token_means = [get_mean_vector(vectorizer2, tokens) for tokens in tokenized_training_words]\ntest_token_means = [get_mean_vector(vectorizer2, tokens) for tokens in tokenized_test_words]","metadata":{"id":"hyV93KDCOmef"},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#get a sample\nprint(train_token_means[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HqDWQIejVj-5","outputId":"abee4f59-74d3-4f40-fe94-dca03a7532ae"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"[-0.21084648  0.445389    0.05559346  0.04506045  0.0531324  -0.6494761\n\n  0.1931807   0.90254235 -0.43405607 -0.28759435 -0.19669372 -0.659014\n\n -0.20870633  0.32652533  0.09727406 -0.26316154  0.13595487 -0.36503133\n\n -0.07884301 -1.1389375   0.290794    0.16797522  0.29810777 -0.12711196\n\n -0.13246979  0.03279702 -0.3087925  -0.23891045 -0.27761534 -0.01645306\n\n  0.42346478 -0.02779581  0.16372022 -0.6320245  -0.08380583  0.5589235\n\n  0.12493669 -0.18918139 -0.33667296 -0.6767805   0.14252342 -0.43321875\n\n -0.24966075  0.03969884  0.4288661  -0.2743494  -0.37856305  0.05805048\n\n  0.2928564   0.41777518  0.1469517  -0.3204251  -0.08994652  0.1990933\n\n -0.11836198  0.13418795  0.25779444 -0.04896947 -0.44929236  0.11987397\n\n  0.1331235   0.17214713  0.03372408 -0.02635223 -0.475709    0.40393704\n\n  0.0297726   0.2655269  -0.54664475  0.62844396 -0.1733785   0.4054786\n\n  0.45923078 -0.09581389  0.4676061   0.02291599  0.1467095  -0.08983044\n\n -0.22206588  0.02735656 -0.3057152  -0.10680708 -0.45181414  0.46780673\n\n -0.15005444 -0.08069077  0.42132804  0.38832137  0.46560237  0.35534173\n\n  0.5096558   0.17193247  0.04091259  0.16763607  0.76680964  0.5083\n\n  0.23836699 -0.38666087  0.20347103 -0.00182271]\n"}]},{"cell_type":"markdown","source":"**Classifier Selection:** Logistic Regression\n\nLogistic Regression a simple yet powerful classification algorithm. It has low computational requirements and generally performing well on binary classification tasks.\n\nI won't dive into the details of logistic regression because there are more than a plenty of them. I will leave a [link](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) for those who are not familiar with.\n\nWhich one performs better LR or NB?\n\nThe Naive assumption does not fit well in large texts. Therefore, LR is more powerful.","metadata":{"id":"9ZFzjn2SXcoy"}},{"cell_type":"code","source":"#define and train a default LR\nmodel = LogisticRegression()\nmodel.fit(train_token_means,y_train)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"Fa3IF-X-BYHi","outputId":"bc8bf87a-6b2b-4fc6-eb75-47429aec1c78"},"execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":["LogisticRegression()"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"]},"metadata":{}}]},{"cell_type":"code","source":"model.score(test_token_means,y_test)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Efn9HBciVv7d","outputId":"48a6d912-bc8a-44a6-bd68-d446de156feb"},"execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":["0.8637015781922525"]},"metadata":{}}]}]}