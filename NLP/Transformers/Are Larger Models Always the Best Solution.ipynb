{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport tensorflow_hub as hub\nimport random\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification,DataCollatorWithPadding\nfrom datasets import Dataset\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom tensorflow.keras import mixed_precision\nmixed_precision.set_global_policy('mixed_float16')\n\nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-22T04:29:38.225641Z","iopub.execute_input":"2023-11-22T04:29:38.226753Z","iopub.status.idle":"2023-11-22T04:29:54.283726Z","shell.execute_reply.started":"2023-11-22T04:29:38.226719Z","shell.execute_reply":"2023-11-22T04:29:54.282836Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Introduction and Intition\n\n\nNovadays using large language models is in a trend. Nothing suprising, since they perform really well especially on big datasets. For some time, I was also working on this competition wanted to share my experiences. I do belive that hands-on experience is at least as important as the theoretical knowledge.\n\n\nTo this end, we will look at four models today. Two of which will be baseline models (Naive Bayes and Logistic Regression), one will be a feature extraction model and the last one is a fine tuned transformer model. I uploaded the predictions of each one to the system and got the accuracy results. I will share the results with you also.\n\nA couple notes:\n\n* There are only one transformer and feature exraction model in this notebook but actually I tried many of them in different configurations. My results ranged somewhere between 0.78 to 0.83.\n\n* I have also tried some prompt engineering staff and its f1-score was 0.79 or so but I do belive that it could be improved since I'm a novice in that area. If I could improvey score I'll update this notebook.\n\n* If you only have to select one baseline model, go with the logistic regression (for binary classification of course). Naive assumption does not go hand in hand with bigger datasets. (More like a side note)\n\nHere are a couple of bullet points from experiences:\n\n* Do not underestimate the power of baseline models. Always be aware of this tradeoff: As machine learning model gets bigger, the required computational resources and  training time also increase. This may not be feasible for many applications. (The thing that I have experienced does not have to be true for all cases. However, it is always good to start with baseline models.)\n\n* There is a method in which you fine-tune the model using masked language modeling first then use the same pre-trained model for sequence classification. The intention is allowing the model to abstract the text in consideration better. Worth for trying but be aware of the risk of overfitting. I did it for `DistilBert` but it did not enhance the results (0.79).\n\n* Mirror Strategy fastens up the fine-tuning process a lot. On the other hand, using TPU nodes can be tricky as defined in the [HuggingFace website](https://huggingface.co/docs/transformers/perf_train_tpu_tf). You can also try XLA with TensorFlow but do not try to run XLA and MirrorStrategy together (1) and do not try XLA with TPU [(2)](https://huggingface.co/docs/transformers/perf_train_tpu_tf).\n\n\nI hope you will find something useful in this work and please share your suggestions if you have any. In the end, the goal should be to learn something from each other...","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:29:54.285801Z","iopub.execute_input":"2023-11-22T04:29:54.286603Z","iopub.status.idle":"2023-11-22T04:29:54.350883Z","shell.execute_reply.started":"2023-11-22T04:29:54.286482Z","shell.execute_reply":"2023-11-22T04:29:54.349999Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:29:54.352076Z","iopub.execute_input":"2023-11-22T04:29:54.352436Z","iopub.status.idle":"2023-11-22T04:29:54.382209Z","shell.execute_reply.started":"2023-11-22T04:29:54.352401Z","shell.execute_reply":"2023-11-22T04:29:54.381358Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Baseline Model 1: Naive Bayes","metadata":{}},{"cell_type":"code","source":"#tokenize and vectorize the datasets using TF-IDF\nvectorizer = TfidfVectorizer()\nvectorizer.fit(train.text)\n\ntrain_text = vectorizer.transform(train.text)\ntest_text = vectorizer.transform(test.text)\n\n#build,train and get predictions of the model\nmodel = MultinomialNB()\nmodel.fit(train_text,train.target)\npreds = model.predict(test_text)\n\n#submission = pd.DataFrame()\n#submission['id'] = test.id\n#submission['target'] = preds\n\n#submission.set_index('id').to_csv('NaiveBayes.csv')\n#0.793","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:29:54.383374Z","iopub.execute_input":"2023-11-22T04:29:54.383723Z","iopub.status.idle":"2023-11-22T04:29:54.849304Z","shell.execute_reply.started":"2023-11-22T04:29:54.383688Z","shell.execute_reply":"2023-11-22T04:29:54.848264Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Model 2: Logistic Regression","metadata":{}},{"cell_type":"code","source":"#build,train and get predictions of the model\nmodel = LogisticRegression()\nmodel.fit(train_text,train.target)\npreds = model.predict(test_text)\n\n#submission = pd.DataFrame()\n#submission['id'] = test.id\n#submission['target'] = preds\n\n#submission.set_index('id').to_csv('LogisticRegression.csv')\n\n#0.793","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:29:54.853618Z","iopub.execute_input":"2023-11-22T04:29:54.854420Z","iopub.status.idle":"2023-11-22T04:29:55.495539Z","shell.execute_reply.started":"2023-11-22T04:29:54.854390Z","shell.execute_reply":"2023-11-22T04:29:55.494302Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Big Model 1: Neural Network with Transfer Learning","metadata":{}},{"cell_type":"code","source":"#get feature extraction layer\n#feature_extraction_layer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:29:55.497389Z","iopub.execute_input":"2023-11-22T04:29:55.498903Z","iopub.status.idle":"2023-11-22T04:29:55.505556Z","shell.execute_reply.started":"2023-11-22T04:29:55.498848Z","shell.execute_reply":"2023-11-22T04:29:55.504093Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#build a nn model using USE\n\n#inputs = layers.Input(shape = [], dtype = tf.string)\n#x = feature_extraction_layer(inputs,training = False)\n#x = layers.Dense(128,activation = 'swish')(x)\n#outputs = layers.Dense(1,activation = 'sigmoid')(x)\n\n#model = tf.keras.Model(inputs,outputs)\n\n#model.compile(optimizer = 'adam',\n#             loss = 'binary_crossentropy',\n#             metrics = 'accuracy')\n\n#history = model.fit(train.text,train.target,epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:29:55.506943Z","iopub.execute_input":"2023-11-22T04:29:55.507375Z","iopub.status.idle":"2023-11-22T04:29:55.514880Z","shell.execute_reply.started":"2023-11-22T04:29:55.507342Z","shell.execute_reply":"2023-11-22T04:29:55.513598Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#get predictions and submit \n#preds = model.predict(test.text)\n#preds_labels = np.round(preds)\n\n#submission = pd.DataFrame()\n#submission['id'] = test.id\n#submission['target'] = preds_labels.ravel().astype(np.int64)\n#submission.set_index('id').to_csv('NN.csv')\n\n#ranges from 78% to 81% for different model configurations","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:29:55.516556Z","iopub.execute_input":"2023-11-22T04:29:55.517725Z","iopub.status.idle":"2023-11-22T04:29:55.527630Z","shell.execute_reply.started":"2023-11-22T04:29:55.517686Z","shell.execute_reply":"2023-11-22T04:29:55.526488Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Big Model 2: Transformers","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('hkayesh/twitter-disaster-nlp')\ndef tokenize_data(example):\n  return tokenizer(example,\n                   truncation = True,\n          padding = 'max_length',\n          max_length = 256,\n          return_tensors = 'np')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:29:55.529502Z","iopub.execute_input":"2023-11-22T04:29:55.530468Z","iopub.status.idle":"2023-11-22T04:29:56.997489Z","shell.execute_reply.started":"2023-11-22T04:29:55.530421Z","shell.execute_reply":"2023-11-22T04:29:56.996525Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"418d6be1476f46488e6bd8262a36d7f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09efefe4286f4c5c8d179941064969f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b8b330124c43b0891dc9b847854f58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"526b3f2f7ca64995bdc931b42e346fd9"}},"metadata":{}}]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntoken_data = train['text'].progress_apply(tokenize_data)\ntrain['attention_mask'] = token_data.progress_apply(lambda x: x['attention_mask'][0])\ntrain['input_ids'] = token_data.progress_apply(lambda x: x['input_ids'][0])\ntokenized_train_dataset = Dataset.from_pandas(train)\n\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntokenized_test = test['text'].progress_apply(tokenize_data)\ntest['input_ids'] = tokenized_test.progress_apply(lambda x: x['input_ids'][0])\ntest['attention_mask'] = tokenized_test.progress_apply(lambda x: x['attention_mask'][0])\ntokenized_test_dataset = Dataset.from_pandas(test)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:30:28.420555Z","iopub.execute_input":"2023-11-22T04:30:28.420849Z","iopub.status.idle":"2023-11-22T04:30:31.668007Z","shell.execute_reply.started":"2023-11-22T04:30:28.420824Z","shell.execute_reply":"2023-11-22T04:30:31.667052Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 7613/7613 [00:02<00:00, 3577.04it/s]\n100%|██████████| 7613/7613 [00:00<00:00, 314426.18it/s]\n100%|██████████| 7613/7613 [00:00<00:00, 328977.73it/s]\n100%|██████████| 3263/3263 [00:00<00:00, 3563.35it/s]\n100%|██████████| 3263/3263 [00:00<00:00, 293106.33it/s]\n100%|██████████| 3263/3263 [00:00<00:00, 326955.11it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"tf_train = tokenized_train_dataset.to_tf_dataset(columns = ['input_ids','attention_mask'],\n                                    batch_size=8,\n                                    shuffle=True,\n                                    collate_fn=DataCollatorWithPadding(tokenizer),\n                                    label_cols = ['target'])\n\ntf_test = tokenized_test_dataset.to_tf_dataset(columns = ['input_ids','attention_mask'],\n                                              batch_size = 8,\n                                              shuffle = False,\n                                              collate_fn = DataCollatorWithPadding(tokenizer),\n                                              )\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:30:36.229493Z","iopub.execute_input":"2023-11-22T04:30:36.230265Z","iopub.status.idle":"2023-11-22T04:30:45.237361Z","shell.execute_reply.started":"2023-11-22T04:30:36.230234Z","shell.execute_reply":"2023-11-22T04:30:45.236538Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = TFAutoModelForSequenceClassification.from_pretrained('hkayesh/twitter-disaster-nlp',num_labels = 2)\nmodel.compile(optimizer = tf.keras.optimizers.Adam(3e-5),\n             jit_compile = True)\nhistory = model.fit(tf_train,epochs = 3)\n\n\n#strategy = tf.distribute.MirroredStrategy()\n#with strategy.scope():\n#    model = TFAutoModelForSequenceClassification.from_pretrained('hkayesh/twitter-disaster-nlp',num_labels = 2)\n#    model.compile(optimizer = tf.keras.optimizers.Adam(3e-5)\n#    history = model.fit(tf_train,epochs = 3)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:30:58.500855Z","iopub.execute_input":"2023-11-22T04:30:58.501670Z","iopub.status.idle":"2023-11-22T04:35:37.875125Z","shell.execute_reply.started":"2023-11-22T04:30:58.501639Z","shell.execute_reply":"2023-11-22T04:35:37.874186Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/538 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8967e4ed252405d9d0b988cca933d3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tf_model.h5:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89876f26892a4cc5bf7911af35263ac3"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n\nAll the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at hkayesh/twitter-disaster-nlp.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\nWARNING: AutoGraph could not transform <function infer_framework at 0x7c8b68586560> and will run it as-is.\nCause: for/else statement not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n951/951 [==============================] - 121s 76ms/step - loss: 0.3183\nEpoch 2/3\n951/951 [==============================] - 74s 77ms/step - loss: 0.2213\nEpoch 3/3\n951/951 [==============================] - 74s 78ms/step - loss: 0.1388\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = model.predict(tf_test)\nlogits = preds.logits\nprobas = tf.nn.softmax(logits,axis = 1)\nlabels = tf.argmax(probas,axis = 1)\n\nsubmission = pd.DataFrame()\nsubmission['id'] = test.id\nsubmission['target'] = labels.numpy()\nsubmission.set_index('id').to_csv('hugging.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T04:35:37.877460Z","iopub.execute_input":"2023-11-22T04:35:37.878293Z","iopub.status.idle":"2023-11-22T04:35:54.667679Z","shell.execute_reply.started":"2023-11-22T04:35:37.878251Z","shell.execute_reply":"2023-11-22T04:35:54.666726Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"408/408 [==============================] - 16s 28ms/step\n","output_type":"stream"}]}]}