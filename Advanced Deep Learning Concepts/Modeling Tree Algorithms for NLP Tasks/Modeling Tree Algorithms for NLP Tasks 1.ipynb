{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow_decision_forests\n!pip install tfds-nightly -U --quiet","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-07-08T08:28:32.039459Z","iopub.execute_input":"2022-07-08T08:28:32.040285Z","iopub.status.idle":"2022-07-08T08:30:18.696139Z","shell.execute_reply.started":"2022-07-08T08:28:32.040225Z","shell.execute_reply":"2022-07-08T08:30:18.694189Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting tensorflow_decision_forests\n  Downloading tensorflow_decision_forests-0.2.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from tensorflow_decision_forests) (1.0.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tensorflow_decision_forests) (1.16.0)\nCollecting tensorflow~=2.9.1\n  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tensorflow_decision_forests) (1.21.6)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from tensorflow_decision_forests) (0.37.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from tensorflow_decision_forests) (1.3.5)\nCollecting wurlitzer\n  Downloading wurlitzer-3.0.2-py3-none-any.whl (7.3 kB)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (59.8.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (3.1.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (3.3.0)\nCollecting tensorboard<2.10,>=2.9\n  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.14.1)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (0.2.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.6.3)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.43.0)\nCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.1.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (21.3)\nCollecting libclang>=13.0.0\n  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.12)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (4.1.1)\nCollecting keras<2.10.0,>=2.9.0rc0\n  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.1.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->tensorflow_decision_forests) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->tensorflow_decision_forests) (2022.1)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow~=2.9.1->tensorflow_decision_forests) (1.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (2.27.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (2.1.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.35.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.4.6)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.6.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.8.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.3.7)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow~=2.9.1->tensorflow_decision_forests) (3.0.9)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (4.11.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (2.0.12)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.26.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.2.0)\nInstalling collected packages: libclang, keras, wurlitzer, tensorflow-io-gcs-filesystem, tensorflow-estimator, protobuf, tensorboard, tensorflow, tensorflow_decision_forests\n  Attempting uninstall: keras\n    Found existing installation: keras 2.6.0\n    Uninstalling keras-2.6.0:\n      Successfully uninstalled keras-2.6.0\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.6.0\n    Uninstalling tensorflow-estimator-2.6.0:\n      Successfully uninstalled tensorflow-estimator-2.6.0\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.1\n    Uninstalling protobuf-3.20.1:\n      Successfully uninstalled protobuf-3.20.1\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.6.0\n    Uninstalling tensorboard-2.6.0:\n      Successfully uninstalled tensorboard-2.6.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.6.4\n    Uninstalling tensorflow-2.6.4:\n      Successfully uninstalled tensorflow-2.6.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.8.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.48.0 which is incompatible.\ntfx-bsl 1.8.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.8.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.8.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5, but you have tensorflow 2.9.1 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.9.1 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.26.0 which is incompatible.\ngrpcio-status 1.46.3 requires grpcio>=1.46.3, but you have grpcio 1.43.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-cloud-storage<2.0.0dev,>=1.26.0, but you have google-cloud-storage 2.1.0 which is incompatible.\nearthengine-api 0.1.315 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 2.48.0 which is incompatible.\napache-beam 2.39.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\napache-beam 2.39.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\napache-beam 2.39.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 8.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.9.0 libclang-14.0.1 protobuf-3.19.4 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 tensorflow_decision_forests-0.2.6 wurlitzer-3.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# The Goal of This Notebook\n\nI enjoy using tree-based algorithms for data science. They are successful, interpretable, easy to use, relatively easy to tune, etc. However, they do not apply to a wide range of tasks such as image preprocessing, NLP tasks, and signal processing.\n\nIn this notebook, I wanted to show how to model NLP tasks for tree-based algorithms with the help of deep learning layers.\n\nThere is also an example on the TensorFlow website about the same topic which you can find [here](https://www.tensorflow.org/decision_forests/tutorials/intermediate_colab).\n\n# The Inspiration\n\nThe inspiration for this kernel is coming from [this](https://arxiv.org/pdf/2009.09991.pdf) article. If you are curious about this topic I highly recommend checking that out.\n\n# The Methodology\n\nI used gradient boosting trees and ensemble trees for a binary text classification task. The hyperparameters of the tree algorithms come from the article. I also included a couple of different deep learning algorithms for comparison. Hope you'll enjoy it!\n\n# The Content\n\n1. [Native Categorical Set Handling](#1)\n2. [Pretrained Embedding](#2)\n3. [Count Based Preprocessing](#3)\n4. [Nontrained Embedding](#4)\n\n[Final Words](#5)","metadata":{}},{"cell_type":"code","source":"import tensorflow_decision_forests as tfdf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-07-08T08:30:31.941508Z","iopub.execute_input":"2022-07-08T08:30:31.942008Z","iopub.status.idle":"2022-07-08T08:30:36.926781Z","shell.execute_reply.started":"2022-07-08T08:30:31.941954Z","shell.execute_reply":"2022-07-08T08:30:36.925349Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2022-07-08 08:30:32.145855: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2022-07-08 08:30:32.145914: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\ndataset = tfds.load('imdb_reviews',\n                          as_supervised=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T08:30:36.930888Z","iopub.execute_input":"2022-07-08T08:30:36.932462Z","iopub.status.idle":"2022-07-08T08:31:37.357876Z","shell.execute_reply.started":"2022-07-08T08:30:36.932417Z","shell.execute_reply":"2022-07-08T08:31:37.356952Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2022-07-08 08:30:37.887451: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95e1dde1bca140adbb1353c1a6fa3fcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef2903b41b14182816cc86c5e31693f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteMGLSIH/imdb_reviews-train.tfrecord*...…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteMGLSIH/imdb_reviews-test.tfrecord*...:…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteMGLSIH/imdb_reviews-unsupervised.tfrec…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\u001b[1mDataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"2022-07-08 08:31:37.204160: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2022-07-08 08:31:37.204225: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n2022-07-08 08:31:37.204296: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (948127bc3417): /proc/driver/nvidia/version does not exist\n2022-07-08 08:31:37.204675: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_ds = dataset[\"train\"].batch(100)\ntest_ds = dataset[\"test\"].batch(100)\n\ntrain_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\ntest_ds = test_ds.cache().prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T08:31:37.359167Z","iopub.execute_input":"2022-07-08T08:31:37.360415Z","iopub.status.idle":"2022-07-08T08:31:37.370297Z","shell.execute_reply.started":"2022-07-08T08:31:37.360374Z","shell.execute_reply":"2022-07-08T08:31:37.368842Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for example, label in train_ds.take(3):\n  for i in range(3):\n    print(example[i])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-07-08T08:31:37.371967Z","iopub.execute_input":"2022-07-08T08:31:37.372322Z","iopub.status.idle":"2022-07-08T08:31:37.493450Z","shell.execute_reply.started":"2022-07-08T08:31:37.372283Z","shell.execute_reply":"2022-07-08T08:31:37.492189Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\ntf.Tensor(b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.', shape=(), dtype=string)\ntf.Tensor(b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.', shape=(), dtype=string)\ntf.Tensor(b\"I enjoyed this movie,and after watching it,it made me wonder just how many 'Caitlin Rose's' exist in the world.How many other girls have been subjected to this sort of sexual abuse,and torment by classmates and have been too frightened to open their mouth about it? Just how threatening and cruel can teenagers be towards one another,because as this film demonstrates,who's right is not foremost important,its who is popular,and feared which manipulates the minds of youths,and influences them to allow this sort of immorality to happen.Tiffani Amber Thiessen gives a powerful performance as the rape victim,and Brian Austin Green is convincing as the guy torn between the girl he thought he loved,and his best friend.This is the kind of film that doesn't get the exposure it deserves.Remarkable,and brilliant,too good to be just a film made for TV.\", shape=(), dtype=string)\ntf.Tensor(b'As a physics student, I\\'ve become aware of many idiot professors, and other so-called experts, in the field. As I continue with my studies, I learn more and more about real physics experiments going on, and about the people who are doing things right.<br /><br />Then, my friends tell me of this \"physics movie\" they want to see. Knowing nothing of it, I\\'m excited, hoping that the information will be presented well.<br /><br />I\\'ve done REAL quantum mechanics; this wasn\\'t it.<br /><br />This movie starts with the basic assumption that anything that occurs to a subatomic particle can, and will, occur to you, if you just open your eyes. Let\\'s think about that, for just a moment.<br /><br />Our bodies are composed of somewhere around 10^30 such subatomic particles. That is a million billion billion billion particles! The more \"mysterious\" quantum effects of just two particles can have a 50% probability of cancelling each other out completely. As you add more and more particles into the mix, it becomes almost impossible to have a large net quantum result. To tell us to believe that this is a valid assumption, with no rationality behind it...it\\'s just stupid.<br /><br />My friend, also in physics, and I counted 3 facts during the course of this movie. But they were presented in the most misleading manner I\\'ve EVER SEEN.<br /><br />I cannot say as much for the neural portion of the movie, as I have not had any kind of medical training. It seemed as though it might have had a slight bit more truth to it, remembering my days in biology, but I cannot say.<br /><br />At least this film had a redeeming quality: the dancing peptides (or whatever they actually were) scene. Not to ruin the invaluable plot that drives this movie, but the main character goes to a wedding, where she sees all different types of personalities \"driven\" by their peptides*, and then the film cuts to the dance floor, where we are spliced between people dancing, sometimes surrounded by CG peptides, and a fully CG scene, filled with dancing peptides. The film, at that point, was trying to tell us how we\\'re \"addicted to emotions,\" so we\\'re treated to the full song of that smash hit, \"Addicted to Love.\"<br /><br />This scene was redeeming, because anyone who could go through THAT scene, and still take this movie seriously...well, you are the ones that need to \"open your eyes.\"', shape=(), dtype=string)\ntf.Tensor(b'This is the last Dutch language film Paul Verhoeven made before going on to make mainstream Hollywood films \"Basic Instinct,\" \"Robocop,\" and \"Total Recall,\" among others. He sets the stage by opening this story with a black widow spider catching prey in her web before we meet Gerard Reve, an annoying self-centered writer with a morbid imagination. Gerard has been invited to be the guest speaker at a Literary Club meeting in sea-side town an hour or so from Amsterdam. Verhoeven lets us have glimpses of how Gerard\\'s imagination twists reality. Asked if writers are a bit close to insanity he admits when he reads the newspaper \"and it says \\'boom\\' I read \\'doom,\\' when it says \\'flood\\' I read \\'blood,\\' when it says \\'red\\' I see \\'dead.\\'\" When he tells a story enough times he begins to believe it; \"I lie the truth.\" He accepts an offer to be the overnight guest of the Club treasurer, a beautiful wealthy salon owner. As he gets to know her and learns her husband has died, he begins to imagine she is \\'a black widow.\\' Is this his more of his reality twist or is she a murderess? This is a psychological drama and in recounting which of these old films have stuck in my memory, I figured out is my favorite gender. Looking at his body of work it is seems to be Paul Verhoeven\\'s too, and he is a master in making us question our own understanding of reality. It\\'s a nice change of pace from the usual Hollywood fare. I saw it in 1983 and it is a film that \"stuck.\"', shape=(), dtype=string)\ntf.Tensor(b\"This movie was bad to say the least!!! The plethora of superb cars are disgraced to have have been showcased in this LAME movie. It starts off with a race from L.A. to Las Vegas to be completed in 1HR 45min...in a Ferrari F430. I did that in 1HR 50min in a tiny 4cylinder 140HP 1993 Honda Accord. Seriously...this movie does not do justice to these cars. Obviously these writers are just that and probably drive under the speed limit with their hands @ 10 & 2 o'clock. I remember seeing on the news how Eddie Griffin crashed a 1.5 million dollar Ferrari Enzo going 30-40MPH>>> youtube.com/watch?v=cNVrMZX2kms <br /><br />And...the director ANDY CHENG is THE biggest SELLOUT!!! He brings shame to his own race. I wonder just how many people he orally pleased just to break into Hollywood. He partook in a movie that portrays Asian Americans in such a negative and FALSE way. Asian women>>cheap money grubbing whores. Asian men>>losing compulsive gamblers & thugs that get beat all the time . What the heck is all the fear about asians?? Why the need to always portray them in such a negative connotation?? I am SO sick of the way Hollywood ALWAYS portrays asians in SUCH a negative and false pretext.\", shape=(), dtype=string)\ntf.Tensor(b\"I don't know what else to say about this horrible movie that hasn't already been said. Honestly I have only myself to be angry with. I should have know better when I saw the title of this movie that it would be a horrible piece of crap, but I loved War Games so I indulged my whim. I will live to regret that decision the rest of my life. From the very start when the government people explained that their super computer could determine who a terrorist was just by how well they played a video game I knew I was in for a ride though the land-that-good-writing-forgot. The list of very, very, very bad plot lines, dialog, and acting is so long I would crash IMDb if I tried to post it. To those people who said that they have seen worst movies than this one please tell me. I am actually curious to see something that could top this steaming pile of horse dung.\", shape=(), dtype=string)\ntf.Tensor(b'Here in Brazil is very rare to see a good Brazilian film, and Brant\\xc2\\xb4s new film is exactly one of these jewel. There are some flaws in the film, of course, but they are very minimal. The directing and acting in this film are very good!<br /><br />Can\\xc2\\xb4t wait to see another Brant\\xc2\\xb4s new film!', shape=(), dtype=string)\n","output_type":"stream"},{"name":"stderr","text":"2022-07-08 08:31:37.459320: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n2022-07-08 08:31:37.459420: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id='1'></a>\n# 1. Native Categorical Set Handling\n\n* Tensorflow Decision forests can handle categorical set of features natively[[1]](https://www.tensorflow.org/decision_forests/tutorials/intermediate_colab). Let's see the performance of native handling.","metadata":{}},{"cell_type":"code","source":"rf_params = {\n    \"num_trees\": 500,\n    \"max_depth\":32,\n    \"categorical_algorithm\":\"RANDOM\",\n    'random_seed':123\n    \n}\n\nmodel_1 = tfdf.keras.RandomForestModel(**rf_params)\nmodel_1.fit(x=train_ds)\nmodel_1.compile(metrics=[\"accuracy\"])\nevaluation = model_1.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T08:31:37.495203Z","iopub.execute_input":"2022-07-08T08:31:37.496408Z","iopub.status.idle":"2022-07-08T08:31:50.010953Z","shell.execute_reply.started":"2022-07-08T08:31:37.496367Z","shell.execute_reply":"2022-07-08T08:31:50.008880Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Use /tmp/tmpewg437zt as temporary training directory\nReading training dataset...\nTraining dataset read in 0:00:07.074143. Found 25000 examples.\nTraining model...\nModel trained in 0:00:00.906747\nCompiling model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO kernel.cc:1176] Loading model from path /tmp/tmpewg437zt/model/ with prefix 7f0901fca6da4225\n[INFO abstract_model.cc:1246] Engine \"RandomForestOptPred\" built\n[INFO kernel.cc:1022] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f1044709200> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nModel compiled.\n250/250 [==============================] - 2s 6ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n","output_type":"stream"}]},{"cell_type":"code","source":"gb_params = {\"max_depth\":6,\n            \"shrinkage\" : 0.1,\n            \"sampling_method\" : None,\n            \"validation_ratio\" : 0.1,\n            \"num_trees\":500}\n\nmodel_1_mart = tfdf.keras.GradientBoostedTreesModel(**gb_params)\nmodel_1_mart.fit(train_ds)\nmodel_1_mart.compile(metrics=[\"accuracy\"])\nevaluation = model_1_mart.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T08:31:50.013515Z","iopub.execute_input":"2022-07-08T08:31:50.013970Z","iopub.status.idle":"2022-07-08T08:31:51.470813Z","shell.execute_reply.started":"2022-07-08T08:31:50.013931Z","shell.execute_reply":"2022-07-08T08:31:51.469858Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Use /tmp/tmpeo4zw5la as temporary training directory\nReading training dataset...\nTraining dataset read in 0:00:00.271786. Found 25000 examples.\nTraining model...\nModel trained in 0:00:00.308375\nCompiling model...\nModel compiled.\n","output_type":"stream"},{"name":"stderr","text":"[INFO kernel.cc:1176] Loading model from path /tmp/tmpeo4zw5la/model/ with prefix 98acda81e1f4433c\n[INFO kernel.cc:1022] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"250/250 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id='2'></a>\n# 2. Pretrained Embedding\n​\n* While using neural nets, we generally compute the algorithm for several epochs due to the nature of SGD. On the other hand, for tree-based algorithms using one epoch is enough for training [[2]](https://www.tensorflow.org/decision_forests/migration). Using pre-trained embedding is a wise move for NLP modeling because embedding weights are not able to update during epochs [[3]](https://www.tensorflow.org/decision_forests/text_features).\n​\n* It's important to note that tree algorithms can not utilize semantic information like neural nets because they try to solve the problems via splitting. They don't utilize matrix multiplication or dot products. Although this is the case, they still perform well most of the time because of all the information that embedding has mostly not used anyways.","metadata":{}},{"cell_type":"code","source":"hub_url = \"http://tfhub.dev/google/universal-sentence-encoder/4\"\nembedding = hub.KerasLayer(hub_url)\n\nsentence = tf.keras.layers.Input(shape = (), name = 'sentence', dtype = tf.string)\nembedded_sentence = embedding(sentence)\npreprocessor = tf.keras.Model(sentence,embedded_sentence)\nmodel_2 = tfdf.keras.RandomForestModel(preprocessing = preprocessor,\n                                     **rf_params)\nmodel_2.fit(train_ds)\nmodel_2.compile(metrics=[\"accuracy\"])\nmodel_2.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T08:31:51.472035Z","iopub.execute_input":"2022-07-08T08:31:51.472686Z","iopub.status.idle":"2022-07-08T08:36:10.222008Z","shell.execute_reply.started":"2022-07-08T08:31:51.472648Z","shell.execute_reply":"2022-07-08T08:36:10.221002Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Use /tmp/tmp_2fbdnur as temporary training directory\nReading training dataset...\nTraining dataset read in 0:00:40.068571. Found 25000 examples.\nTraining model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO kernel.cc:1176] Loading model from path /tmp/tmp_2fbdnur/model/ with prefix ef13ae50cbbb4117\n[INFO abstract_model.cc:1246] Engine \"RandomForestOptPred\" built\n[INFO kernel.cc:1022] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"Model trained in 0:02:32.919475\nCompiling model...\nModel compiled.\n250/250 [==============================] - 40s 152ms/step - loss: 0.0000e+00 - accuracy: 0.8382\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[0.0, 0.8382400274276733]"},"metadata":{}}]},{"cell_type":"code","source":"model_2_mart = tfdf.keras.GradientBoostedTreesModel(**gb_params,\n                                                    preprocessing = preprocessor)\nmodel_2_mart.fit(train_ds)\nmodel_2_mart.compile(metrics=[\"accuracy\"])\nmodel_2_mart.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T08:36:10.225328Z","iopub.execute_input":"2022-07-08T08:36:10.225981Z","iopub.status.idle":"2022-07-08T08:46:27.152326Z","shell.execute_reply.started":"2022-07-08T08:36:10.225946Z","shell.execute_reply":"2022-07-08T08:46:27.150217Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Use /tmp/tmpkcgden5f as temporary training directory\nReading training dataset...\nTraining dataset read in 0:00:40.878853. Found 25000 examples.\nTraining model...\nModel trained in 0:08:53.112875\nCompiling model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO kernel.cc:1176] Loading model from path /tmp/tmpkcgden5f/model/ with prefix cbb295e182d24030\n[INFO abstract_model.cc:1246] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n[INFO kernel.cc:1022] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"Model compiled.\n250/250 [==============================] - 37s 141ms/step - loss: 0.0000e+00 - accuracy: 0.8526\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[0.0, 0.8526399731636047]"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id='3'></a>\n# 3. Count Based Preprocessing\n\n* Count-based preprocessing relies on counting the number of times the token at that index appeared in the batch item [[4]](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization). In the article, this preprocessing is named \"BagOfWord\". You can find the relative information on page 2 at the top of column 2.","metadata":{}},{"cell_type":"code","source":"layer = tf.keras.layers.TextVectorization()\nlayer.adapt(train_ds.map(lambda x,y:x))\nprint(\"Number of words: \",len(layer.get_vocabulary()))\n\nMAX_TOKENS = 5000\n\ncount_layer = tf.keras.layers.TextVectorization(\n                                    max_tokens=MAX_TOKENS, output_mode = 'count'\n                                     )\ncount_layer.adapt(train_ds.map(lambda x,y:x))\n\nsentence = tf.keras.layers.Input(shape = (), name = 'sentence', dtype = tf.string)\nencoded_sentence = count_layer(sentence)\npreprocess_model = tf.keras.Model(sentence,encoded_sentence)\n\nmodel_3 = tfdf.keras.RandomForestModel(preprocessing = preprocess_model,\n                                     **rf_params)\nmodel_3.fit(train_ds)\nmodel_3.compile(metrics=[\"accuracy\"])\nmodel_3.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T08:46:27.155415Z","iopub.execute_input":"2022-07-08T08:46:27.157240Z","iopub.status.idle":"2022-07-08T09:11:32.287054Z","shell.execute_reply.started":"2022-07-08T08:46:27.157181Z","shell.execute_reply":"2022-07-08T09:11:32.286114Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Number of words:  121894\nUse /tmp/tmps2pgr1u8 as temporary training directory\nReading training dataset...\nTraining dataset read in 0:01:11.667403. Found 25000 examples.\nTraining model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO kernel.cc:1176] Loading model from path /tmp/tmps2pgr1u8/model/ with prefix a64ae104f5d74587\n[INFO abstract_model.cc:1246] Engine \"RandomForestOptPred\" built\n[INFO kernel.cc:1022] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"Model trained in 0:22:17.919885\nCompiling model...\nModel compiled.\n250/250 [==============================] - 34s 53ms/step - loss: 0.0000e+00 - accuracy: 0.8241\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[0.0, 0.8240799903869629]"},"metadata":{}}]},{"cell_type":"code","source":"model_3_mart = tfdf.keras.GradientBoostedTreesModel(**gb_params,\n                                                    preprocessing = preprocess_model,\n                                                    )\nmodel_3_mart.fit(train_ds)\nmodel_3_mart.compile(metrics=[\"accuracy\"])\nmodel_3_mart.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:11:32.288441Z","iopub.execute_input":"2022-07-08T09:11:32.289260Z","iopub.status.idle":"2022-07-08T10:21:41.754153Z","shell.execute_reply.started":"2022-07-08T09:11:32.289207Z","shell.execute_reply":"2022-07-08T10:21:41.752744Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Use /tmp/tmpa5w9qkbu as temporary training directory\nReading training dataset...\nTraining dataset read in 0:01:07.693841. Found 25000 examples.\nTraining model...\nModel trained in 1:07:37.362400\nCompiling model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO kernel.cc:1176] Loading model from path /tmp/tmpa5w9qkbu/model/ with prefix 74239e1c48bc4619\n[INFO abstract_model.cc:1246] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n[INFO kernel.cc:1022] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"Model compiled.\n250/250 [==============================] - 27s 30ms/step - loss: 0.0000e+00 - accuracy: 0.8692\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[0.0, 0.8692399859428406]"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id='4'></a>\n# 4. Nontrained Embedding \n\n* I prepared this part just to prove why using pre-trained embedding is a better idea.","metadata":{}},{"cell_type":"code","source":"sentence = tf.keras.Input(shape = ())\nindexer = tf.keras.layers.TextVectorization(max_tokens = MAX_TOKENS,\n                                            output_mode = 'int',\n                                            output_sequence_length = MAX_TOKENS)\nindexer.adapt(train_ds.map(lambda x,y: x))\n\nembedding = tf.keras.layers.Embedding(input_dim = 5000, output_dim = 512)\n\nsentence = tf.keras.Input(shape = (), name = 'sentence', dtype = tf.string)\nindexed_sentence = indexer(sentence)\nembedded_sentence = embedding(indexed_sentence)\noutput = tf.keras.layers.GlobalAveragePooling1D()(embedded_sentence)\n\nnon_trained_embedding_model = tf.keras.Model(sentence,output)\n\nmodel_4 = tfdf.keras.RandomForestModel(preprocessing = non_trained_embedding_model,\n                                     **rf_params)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-08T10:21:41.756193Z","iopub.execute_input":"2022-07-08T10:21:41.756707Z","iopub.status.idle":"2022-07-08T10:21:44.515074Z","shell.execute_reply.started":"2022-07-08T10:21:41.756660Z","shell.execute_reply":"2022-07-08T10:21:44.514264Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Use /tmp/tmpf6wm3aff as temporary training directory\n","output_type":"stream"}]},{"cell_type":"code","source":"model_4.fit(train_ds)\nmodel_4.compile(metrics=[\"accuracy\"])\nmodel_4.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T10:21:44.516354Z","iopub.execute_input":"2022-07-08T10:21:44.516929Z","iopub.status.idle":"2022-07-08T10:29:11.551627Z","shell.execute_reply.started":"2022-07-08T10:21:44.516897Z","shell.execute_reply":"2022-07-08T10:29:11.550639Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Reading training dataset...\n","output_type":"stream"},{"name":"stderr","text":"2022-07-08 10:21:49.696943: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1024000000 exceeds 10% of free system memory.\n2022-07-08 10:21:50.621850: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1024000000 exceeds 10% of free system memory.\n2022-07-08 10:21:51.058738: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1024000000 exceeds 10% of free system memory.\n2022-07-08 10:21:51.489772: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1024000000 exceeds 10% of free system memory.\n2022-07-08 10:21:51.936681: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1024000000 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"Training dataset read in 0:01:56.556941. Found 25000 examples.\nTraining model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO kernel.cc:1176] Loading model from path /tmp/tmpf6wm3aff/model/ with prefix 1c150f55bd544f41\n[INFO abstract_model.cc:1246] Engine \"RandomForestOptPred\" built\n[INFO kernel.cc:1022] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"Model trained in 0:03:27.490342\nCompiling model...\nModel compiled.\n250/250 [==============================] - 118s 466ms/step - loss: 0.0000e+00 - accuracy: 0.6735\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[0.0, 0.673520028591156]"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id='5'></a>\n# Final Words\n\n* Let's continue this experiment with different neural network models before wrapping it up.\n\n* You can find the rest of the experiment [here](https://www.kaggle.com/code/egemenuurdalg/modeling-tree-algorithms-for-nlp-tasks-part-2?scriptVersionId=100331891). ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}